{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = {'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 124439808 || all params: 124439808 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DocRED/GPT_w_ner_short_relation_onlyRC/gpt2_tokenizer/tokenizer_config.json',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/gpt2_tokenizer/special_tokens_map.json',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/gpt2_tokenizer/vocab.json',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/gpt2_tokenizer/merges.txt',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/gpt2_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "\n",
    "tokenizer.save_pretrained('DocRED/GPT_w_ner_short_relation_onlyRC/gpt2_tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/train_annotated.json') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vertexSet': [[{'pos': [0, 4],\n",
       "    'type': 'ORG',\n",
       "    'sent_id': 0,\n",
       "    'name': 'Zest Airways, Inc.'},\n",
       "   {'sent_id': 0,\n",
       "    'type': 'ORG',\n",
       "    'pos': [10, 15],\n",
       "    'name': 'Asian Spirit and Zest Air'},\n",
       "   {'name': 'AirAsia Zest', 'pos': [6, 8], 'sent_id': 0, 'type': 'ORG'},\n",
       "   {'name': 'AirAsia Zest', 'pos': [19, 21], 'sent_id': 6, 'type': 'ORG'}],\n",
       "  [{'name': 'Ninoy Aquino International Airport',\n",
       "    'pos': [4, 8],\n",
       "    'sent_id': 3,\n",
       "    'type': 'LOC'},\n",
       "   {'name': 'Ninoy Aquino International Airport',\n",
       "    'pos': [26, 30],\n",
       "    'sent_id': 0,\n",
       "    'type': 'LOC'}],\n",
       "  [{'name': 'Pasay City', 'pos': [31, 33], 'sent_id': 0, 'type': 'LOC'}],\n",
       "  [{'name': 'Metro Manila', 'pos': [34, 36], 'sent_id': 0, 'type': 'LOC'}],\n",
       "  [{'name': 'Philippines', 'pos': [38, 39], 'sent_id': 0, 'type': 'LOC'},\n",
       "   {'name': 'Philippines', 'pos': [13, 14], 'sent_id': 4, 'type': 'LOC'},\n",
       "   {'sent_id': 5,\n",
       "    'type': 'LOC',\n",
       "    'pos': [25, 29],\n",
       "    'name': 'Republic of the Philippines'}],\n",
       "  [{'name': 'Manila', 'pos': [13, 14], 'sent_id': 1, 'type': 'LOC'},\n",
       "   {'name': 'Manila', 'pos': [9, 10], 'sent_id': 3, 'type': 'LOC'}],\n",
       "  [{'name': 'Cebu', 'pos': [15, 16], 'sent_id': 1, 'type': 'LOC'}],\n",
       "  [{'pos': [17, 18], 'type': 'NUM', 'sent_id': 1, 'name': '24'}],\n",
       "  [{'pos': [1, 2], 'type': 'TIME', 'sent_id': 2, 'name': '2013'},\n",
       "   {'pos': [1, 5], 'type': 'TIME', 'sent_id': 5, 'name': 'August 16, 2013'}],\n",
       "  [{'pos': [9, 11],\n",
       "    'type': 'ORG',\n",
       "    'name': 'Philippines AirAsia',\n",
       "    'sent_id': 2}],\n",
       "  [{'pos': [5, 7], 'type': 'ORG', 'sent_id': 4, 'name': 'Asian Spirit'}],\n",
       "  [{'pos': [7, 13],\n",
       "    'type': 'ORG',\n",
       "    'sent_id': 5,\n",
       "    'name': 'Civil Aviation Authority of the Philippines'},\n",
       "   {'name': 'CAAP', 'pos': [14, 15], 'sent_id': 5, 'type': 'ORG'}],\n",
       "  [{'name': 'Zest Air', 'pos': [34, 36], 'sent_id': 5, 'type': 'ORG'},\n",
       "   {'pos': [7, 9], 'type': 'ORG', 'sent_id': 6, 'name': 'Zest Air'}],\n",
       "  [{'sent_id': 6, 'type': 'NUM', 'pos': [2, 4], 'name': 'a year'}],\n",
       "  [{'name': 'AirAsia', 'pos': [5, 6], 'sent_id': 6, 'type': 'ORG'}],\n",
       "  [{'pos': [5, 7],\n",
       "    'type': 'ORG',\n",
       "    'name': 'AirAsia Philippines',\n",
       "    'sent_id': 7}],\n",
       "  [{'pos': [8, 10], 'type': 'TIME', 'sent_id': 7, 'name': 'January 2016'}]],\n",
       " 'labels': [{'r': 'P159', 'h': 0, 't': 2, 'evidence': [0]},\n",
       "  {'r': 'P17', 'h': 0, 't': 4, 'evidence': [2, 4, 7]},\n",
       "  {'r': 'P17', 'h': 12, 't': 4, 'evidence': [6, 7]},\n",
       "  {'r': 'P17', 'h': 2, 't': 4, 'evidence': [0]},\n",
       "  {'r': 'P131', 'h': 2, 't': 3, 'evidence': [0]},\n",
       "  {'r': 'P150', 'h': 4, 't': 3, 'evidence': [0]},\n",
       "  {'r': 'P17', 'h': 5, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P150', 'h': 3, 't': 2, 'evidence': [0]},\n",
       "  {'r': 'P131', 'h': 3, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P17', 'h': 3, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P131', 'h': 1, 't': 2, 'evidence': [0, 3]},\n",
       "  {'r': 'P17', 'h': 1, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P17', 'h': 10, 't': 4, 'evidence': [4]}],\n",
       " 'title': 'AirAsia Zest',\n",
       " 'sents': [['Zest',\n",
       "   'Airways',\n",
       "   ',',\n",
       "   'Inc.',\n",
       "   'operated',\n",
       "   'as',\n",
       "   'AirAsia',\n",
       "   'Zest',\n",
       "   '(',\n",
       "   'formerly',\n",
       "   'Asian',\n",
       "   'Spirit',\n",
       "   'and',\n",
       "   'Zest',\n",
       "   'Air',\n",
       "   ')',\n",
       "   ',',\n",
       "   'was',\n",
       "   'a',\n",
       "   'low',\n",
       "   '-',\n",
       "   'cost',\n",
       "   'airline',\n",
       "   'based',\n",
       "   'at',\n",
       "   'the',\n",
       "   'Ninoy',\n",
       "   'Aquino',\n",
       "   'International',\n",
       "   'Airport',\n",
       "   'in',\n",
       "   'Pasay',\n",
       "   'City',\n",
       "   ',',\n",
       "   'Metro',\n",
       "   'Manila',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   '.'],\n",
       "  ['It',\n",
       "   'operated',\n",
       "   'scheduled',\n",
       "   'domestic',\n",
       "   'and',\n",
       "   'international',\n",
       "   'tourist',\n",
       "   'services',\n",
       "   ',',\n",
       "   'mainly',\n",
       "   'feeder',\n",
       "   'services',\n",
       "   'linking',\n",
       "   'Manila',\n",
       "   'and',\n",
       "   'Cebu',\n",
       "   'with',\n",
       "   '24',\n",
       "   'domestic',\n",
       "   'destinations',\n",
       "   'in',\n",
       "   'support',\n",
       "   'of',\n",
       "   'the',\n",
       "   'trunk',\n",
       "   'route',\n",
       "   'operations',\n",
       "   'of',\n",
       "   'other',\n",
       "   'airlines',\n",
       "   '.'],\n",
       "  ['In',\n",
       "   '2013',\n",
       "   ',',\n",
       "   'the',\n",
       "   'airline',\n",
       "   'became',\n",
       "   'an',\n",
       "   'affiliate',\n",
       "   'of',\n",
       "   'Philippines',\n",
       "   'AirAsia',\n",
       "   'operating',\n",
       "   'their',\n",
       "   'brand',\n",
       "   'separately',\n",
       "   '.'],\n",
       "  ['Its',\n",
       "   'main',\n",
       "   'base',\n",
       "   'was',\n",
       "   'Ninoy',\n",
       "   'Aquino',\n",
       "   'International',\n",
       "   'Airport',\n",
       "   ',',\n",
       "   'Manila',\n",
       "   '.'],\n",
       "  ['The',\n",
       "   'airline',\n",
       "   'was',\n",
       "   'founded',\n",
       "   'as',\n",
       "   'Asian',\n",
       "   'Spirit',\n",
       "   ',',\n",
       "   'the',\n",
       "   'first',\n",
       "   'airline',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   'to',\n",
       "   'be',\n",
       "   'run',\n",
       "   'as',\n",
       "   'a',\n",
       "   'cooperative',\n",
       "   '.'],\n",
       "  ['On',\n",
       "   'August',\n",
       "   '16',\n",
       "   ',',\n",
       "   '2013',\n",
       "   ',',\n",
       "   'the',\n",
       "   'Civil',\n",
       "   'Aviation',\n",
       "   'Authority',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   '(',\n",
       "   'CAAP',\n",
       "   ')',\n",
       "   ',',\n",
       "   'the',\n",
       "   'regulating',\n",
       "   'body',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Government',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Republic',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   'for',\n",
       "   'civil',\n",
       "   'aviation',\n",
       "   ',',\n",
       "   'suspended',\n",
       "   'Zest',\n",
       "   'Air',\n",
       "   'flights',\n",
       "   'until',\n",
       "   'further',\n",
       "   'notice',\n",
       "   'because',\n",
       "   'of',\n",
       "   'safety',\n",
       "   'issues',\n",
       "   '.'],\n",
       "  ['Less',\n",
       "   'than',\n",
       "   'a',\n",
       "   'year',\n",
       "   'after',\n",
       "   'AirAsia',\n",
       "   'and',\n",
       "   'Zest',\n",
       "   'Air',\n",
       "   \"'s\",\n",
       "   'strategic',\n",
       "   'alliance',\n",
       "   ',',\n",
       "   'the',\n",
       "   'airline',\n",
       "   'has',\n",
       "   'been',\n",
       "   'rebranded',\n",
       "   'as',\n",
       "   'AirAsia',\n",
       "   'Zest',\n",
       "   '.'],\n",
       "  ['The',\n",
       "   'airline',\n",
       "   'was',\n",
       "   'merged',\n",
       "   'into',\n",
       "   'AirAsia',\n",
       "   'Philippines',\n",
       "   'in',\n",
       "   'January',\n",
       "   '2016',\n",
       "   '.']]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthe names of vertextSet can be the same, but the pos should be different\\nstructure:\\n'vertexSet': \\n    [\\n        {\\n            'pos':[start, end],\\n            'type': 'NER',\\n            'sent_id': 0,\\n            'name': 'string',\\n        },\\n        {}\\n    ]\\n'labels':\\n    [\\n        {\\n            'r': 'Pxx',\\n            'h': 0,\\n            't': 1,\\n            'evidence': [2, 3, 4],\\n        },\\n        {}\\n    ]\\n'title': 'string',\\n'sents':\\n    [\\n        ['word0', 'word1',]\\n        ['word0', 'word1',]\\n    ]\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "the names of vertextSet can be the same, but the pos should be different\n",
    "structure:\n",
    "'vertexSet': \n",
    "    [\n",
    "        (for the same entity but in different synonyms and different sentences)\n",
    "        [\n",
    "            {\n",
    "                'pos':[start, end],\n",
    "                'type': 'NER',\n",
    "                'sent_id': 0,\n",
    "                'name': 'string',\n",
    "            },\n",
    "            {}\n",
    "        ],\n",
    "        [entity-2]\n",
    "    ]\n",
    "'labels':\n",
    "    [\n",
    "        {\n",
    "            'r': 'Pxx',\n",
    "            'h': 0,\n",
    "            't': 1,\n",
    "            'evidence': [2, 3, 4],\n",
    "        },\n",
    "        {}\n",
    "    ]\n",
    "'title': 'string',\n",
    "'sents':\n",
    "    [\n",
    "        ['word0', 'word1',]\n",
    "        ['word0', 'word1',]\n",
    "    ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DocRED/data/ner_info.json') as f:\n",
    "    ner_info = json.load(f)\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    relation_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# doc-level is too long for gpt-2, so we need to split the doc-level into bi-sent-level\n",
    "\n",
    "relation_dict = {\n",
    "    'id': [],\n",
    "    'text':[],\n",
    "    'entity': [],\n",
    "    'relation': []\n",
    "}\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    # id\n",
    "    relation_dict['id'].append(i)\n",
    "\n",
    "    # text\n",
    "    sents = \"\"\n",
    "    for sent in train_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # if there are space, delete the first and last space of the sents\n",
    "    sents = sents.strip()\n",
    "    # delete double space in the sents\n",
    "    sents = sents.replace(\"  \", \" \")\n",
    "    relation_dict['text'].append(sents)\n",
    "    del sents\n",
    "\n",
    "    # entity\n",
    "    entity = []\n",
    "    entity_list = []\n",
    "    entity_flat = {}\n",
    "    entity_count = 0\n",
    "    for sent_item in train_set[i]['vertexSet']:\n",
    "        for item in sent_item:\n",
    "            entity_item = []\n",
    "            if item['name'].lower() not in entity_list:\n",
    "                entity_list.append(item['name'].lower().strip())\n",
    "                entity_item.append(item['name'].lower().strip())\n",
    "                entity_item.append(ner_info[item['type']])\n",
    "\n",
    "                entity.append(entity_item)\n",
    "            \n",
    "            # add the entity_flat\n",
    "            entity_flat[entity_count] = item['name'].lower().strip()\n",
    "            entity_count += 1\n",
    "\n",
    "    # release the entity_list and entity_item\n",
    "    del entity_item\n",
    "    del entity_count\n",
    "        \n",
    "\n",
    "    # relation pairs\n",
    "    relation_pairs = {}\n",
    "    for relation_item in train_set[i]['labels']:\n",
    "        pair = []\n",
    "        head = entity_flat[relation_item['h']]\n",
    "        tail = entity_flat[relation_item['t']]\n",
    "        pair.append(head)\n",
    "        pair.append(tail)\n",
    "\n",
    "        relation  = relation_info[relation_item['r']]\n",
    "        if relation not in relation_pairs.keys():\n",
    "            relation_pairs[relation] = []\n",
    "\n",
    "        relation_pairs[relation].append(pair)\n",
    "    del pair\n",
    "    del head\n",
    "    del tail\n",
    "\n",
    "    # add the entity and relation pairs to the relation_dict\n",
    "    relation_dict['entity'].append(entity)\n",
    "    relation_dict['relation'].append(relation_pairs)\n",
    "    break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "# with open('DocRED/data/DocRED_baseline_metadata/relation_dict.json', 'w') as f:\n",
    "#     json.dump(relation_dict, f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {\n",
    "    'text':[],\n",
    "    'entity': [],\n",
    "    'relation': []\n",
    "}\n",
    "id_count = 0\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    \n",
    "\n",
    "    # text\n",
    "    sent_pairs = []\n",
    "    for sent_index, sent in enumerate(train_set[i]['sents']):\n",
    "        sents = \"\"\n",
    "\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "\n",
    "        # and the next_sent if it exists\n",
    "        try:\n",
    "            next_sent = train_set[i]['sents'][sent_index + 1]\n",
    "            b = \" \".join(next_sent)\n",
    "            sents += b.lower() + \" \"\n",
    "        except:\n",
    "            pass\n",
    "        # post process the sents for some spaces\n",
    "        sents = sents.strip()\n",
    "        sents = sents.replace(\"  \", \" \")\n",
    "\n",
    "        relation_dict['text'].append(sents)\n",
    "            \n",
    "    del sents\n",
    "\n",
    "\n",
    "    # entity\n",
    "    entity = []\n",
    "    for index in range(len(train_set[i]['sents'])):\n",
    "        # focus on the current sent and the next sent if it exists\n",
    "        if index + 1 < len(train_set[i]['sents']):\n",
    "            next_index = index + 1\n",
    "        else:\n",
    "            next_index = index\n",
    "        # group the entities for every 2 sents, no repeated entities in one group\n",
    "        c_sent_entity_lists = []\n",
    "        next_sent_entity_lists = []\n",
    "        entity_for_each_2_sents = []\n",
    "\n",
    "        for entity_spans in train_set[i]['vertexSet']:\n",
    "            for item in entity_spans:\n",
    "                entity_item = []\n",
    "                # if neither in the current sent nor in the next sent, continue\n",
    "                if item['sent_id'] != index and item['sent_id'] != next_index:\n",
    "                    continue\n",
    "                # also store the first pos of the entity in the entity_item\n",
    "                # it will look like this: [[entity_name, sent_index, pos1, ner_type]]\n",
    "                entity_item = [item['name'].lower().strip(), item['sent_id'], item['pos'][0], ner_info[item['type']]]\n",
    "\n",
    "                if entity_item[1] == index:\n",
    "                    c_sent_entity_lists.append(entity_item)\n",
    "                else:\n",
    "                    next_sent_entity_lists.append(entity_item)\n",
    "\n",
    "        # sort the c_sent_entity_lists and next_sent_entity_lists by the pos in ascending order\n",
    "        c_sent_entity_lists.sort(key=lambda x: x[2])\n",
    "        if index != next_index:\n",
    "            next_sent_entity_lists.sort(key=lambda x: x[2])\n",
    "        \n",
    "        entity_list = []\n",
    "        for item in c_sent_entity_lists:\n",
    "            if item[0] not in entity_list:\n",
    "                entity_list.append(item[0])\n",
    "                entity_for_each_2_sents.append([item[0], item[3]])\n",
    "            \n",
    "        if index != next_index:\n",
    "            for item in next_sent_entity_lists:\n",
    "                if item[0] not in entity_list:\n",
    "                    entity_list.append(item[0])\n",
    "                    entity_for_each_2_sents.append([item[0], item[3]])\n",
    "\n",
    "        relation_dict['entity'].append(entity_for_each_2_sents)\n",
    "\n",
    "    del entity_item\n",
    "    del c_sent_entity_lists\n",
    "    del next_sent_entity_lists\n",
    "    del entity_for_each_2_sents\n",
    "        \n",
    "\n",
    "    # relation pairs\n",
    "    relation_pairs = []\n",
    "\n",
    "    for index in range(len(train_set[i]['sents'])):\n",
    "        relation_pairs_for_each_2_sents = {}\n",
    "        # focus on the current sent and the next sent if it exists\n",
    "        if index + 1 < len(train_set[i]['vertexSet']):\n",
    "            next_index = index + 1\n",
    "        else:\n",
    "            next_index = index\n",
    "\n",
    "        # heads, tails: ['entity_name', start_pos]\n",
    "        for relation_item in train_set[i]['labels']:\n",
    "            heads = []\n",
    "            tails = []\n",
    "            \n",
    "            # head\n",
    "            head_exist = False\n",
    "            for head_span in train_set[i]['vertexSet'][relation_item['h']]:\n",
    "                if head_span['sent_id'] == index or head_span['sent_id'] == next_index:\n",
    "                    heads.append([head_span['name'].lower().strip(), head_span['pos'][0]])\n",
    "                    head_exist = True\n",
    "            if not head_exist:\n",
    "                continue\n",
    "    \n",
    "            # tail\n",
    "            tail_exist = False\n",
    "            for tail_span in train_set[i]['vertexSet'][relation_item['t']]:\n",
    "                if tail_span['sent_id'] == index or tail_span['sent_id'] == next_index:\n",
    "                    tails.append([tail_span['name'].lower().strip(), tail_span['pos'][0]])\n",
    "                    tail_exist = True\n",
    "            if not tail_exist:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            if relation_info[relation_item['r']] not in relation_pairs_for_each_2_sents.keys():\n",
    "                relation_pairs_for_each_2_sents[relation_info[relation_item['r']]] = []\n",
    "            for head in heads:\n",
    "                for tail in tails:\n",
    "                    relation_pairs_for_each_2_sents[relation_info[relation_item['r']]].append([head[0], tail[0]])\n",
    "\n",
    "        relation_dict['relation'].append(relation_pairs_for_each_2_sents)\n",
    "\n",
    "\n",
    "    \n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "# with open('DocRED/data/bi-sent-pre-process.json', 'w') as f:\n",
    "    # json.dump(relation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_dict['text']) == len(relation_dict['entity']) == len(relation_dict['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headquarters location': [['zest airways, inc.', 'pasay city'],\n",
       "  ['asian spirit and zest air', 'pasay city'],\n",
       "  ['airasia zest', 'pasay city']],\n",
       " 'country': [['zest airways, inc.', 'philippines'],\n",
       "  ['asian spirit and zest air', 'philippines'],\n",
       "  ['airasia zest', 'philippines'],\n",
       "  ['pasay city', 'philippines'],\n",
       "  ['manila', 'philippines'],\n",
       "  ['metro manila', 'philippines'],\n",
       "  ['ninoy aquino international airport', 'philippines']],\n",
       " 'located in the administrative territorial entity': [['pasay city',\n",
       "   'metro manila'],\n",
       "  ['metro manila', 'philippines'],\n",
       "  ['ninoy aquino international airport', 'pasay city']],\n",
       " 'contains administrative territorial entity': [['philippines',\n",
       "   'metro manila'],\n",
       "  ['metro manila', 'pasay city']]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict['relation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ner = 1\n",
    "from datasets import Dataset\n",
    "\n",
    "relation_dict = {}\n",
    "if ner:\n",
    "    with open('DocRED/data/bi-sent-pre-process.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'entity': relation_dict['entity'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'entity', 'relation'],\n",
       "    num_rows: 24256\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'zest airways , inc. operated as airasia zest ( formerly asian spirit and zest air ) , was a low - cost airline based at the ninoy aquino international airport in pasay city , metro manila in the philippines . it operated scheduled domestic and international tourist services , mainly feeder services linking manila and cebu with 24 domestic destinations in support of the trunk route operations of other airlines .',\n",
       " 'entity': [['zest airways, inc.', 'organization'],\n",
       "  ['airasia zest', 'organization'],\n",
       "  ['asian spirit and zest air', 'organization'],\n",
       "  ['ninoy aquino international airport', 'location'],\n",
       "  ['pasay city', 'location'],\n",
       "  ['metro manila', 'location'],\n",
       "  ['philippines', 'location'],\n",
       "  ['manila', 'location'],\n",
       "  ['cebu', 'location'],\n",
       "  ['24', 'number']],\n",
       " 'relation': {'applies to jurisdiction': None,\n",
       "  'author': None,\n",
       "  'award received': None,\n",
       "  'basin country': None,\n",
       "  'capital': None,\n",
       "  'capital of': None,\n",
       "  'cast member': None,\n",
       "  'chairperson': None,\n",
       "  'characters': None,\n",
       "  'child': None,\n",
       "  'composer': None,\n",
       "  'conflict': None,\n",
       "  'contains administrative territorial entity': [['philippines',\n",
       "    'metro manila'],\n",
       "   ['metro manila', 'pasay city']],\n",
       "  'continent': None,\n",
       "  'country': [['zest airways, inc.', 'philippines'],\n",
       "   ['asian spirit and zest air', 'philippines'],\n",
       "   ['airasia zest', 'philippines'],\n",
       "   ['pasay city', 'philippines'],\n",
       "   ['manila', 'philippines'],\n",
       "   ['metro manila', 'philippines'],\n",
       "   ['ninoy aquino international airport', 'philippines']],\n",
       "  'country of citizenship': None,\n",
       "  'country of origin': None,\n",
       "  'creator': None,\n",
       "  'date of birth': None,\n",
       "  'date of death': None,\n",
       "  'developer': None,\n",
       "  'director': None,\n",
       "  'dissolved, abolished or demolished': None,\n",
       "  'educated at': None,\n",
       "  'employer': None,\n",
       "  'end time': None,\n",
       "  'ethnic group': None,\n",
       "  'father': None,\n",
       "  'followed by': None,\n",
       "  'follows': None,\n",
       "  'founded by': None,\n",
       "  'genre': None,\n",
       "  'has part': None,\n",
       "  'head of government': None,\n",
       "  'head of state': None,\n",
       "  'headquarters location': [['zest airways, inc.', 'pasay city'],\n",
       "   ['asian spirit and zest air', 'pasay city'],\n",
       "   ['airasia zest', 'pasay city']],\n",
       "  'inception': None,\n",
       "  'influenced by': None,\n",
       "  'instance of': None,\n",
       "  'languages spoken, written or signed': None,\n",
       "  'league': None,\n",
       "  'legislative body': None,\n",
       "  'located in or next to body of water': None,\n",
       "  'located in the administrative territorial entity': [['pasay city',\n",
       "    'metro manila'],\n",
       "   ['metro manila', 'philippines'],\n",
       "   ['ninoy aquino international airport', 'pasay city']],\n",
       "  'located on terrain feature': None,\n",
       "  'location': None,\n",
       "  'location of formation': None,\n",
       "  'lyrics by': None,\n",
       "  'manufacturer': None,\n",
       "  'member of': None,\n",
       "  'member of political party': None,\n",
       "  'member of sports team': None,\n",
       "  'military branch': None,\n",
       "  'mother': None,\n",
       "  'mouth of the watercourse': None,\n",
       "  'narrative location': None,\n",
       "  'notable work': None,\n",
       "  'official language': None,\n",
       "  'operator': None,\n",
       "  'original language of work': None,\n",
       "  'original network': None,\n",
       "  'owned by': None,\n",
       "  'parent organization': None,\n",
       "  'parent taxon': None,\n",
       "  'part of': None,\n",
       "  'participant': None,\n",
       "  'participant of': None,\n",
       "  'performer': None,\n",
       "  'place of birth': None,\n",
       "  'place of death': None,\n",
       "  'platform': None,\n",
       "  'point in time': None,\n",
       "  'position held': None,\n",
       "  'present in work': None,\n",
       "  'producer': None,\n",
       "  'product or material produced': None,\n",
       "  'production company': None,\n",
       "  'publication date': None,\n",
       "  'publisher': None,\n",
       "  'record label': None,\n",
       "  'religion': None,\n",
       "  'replaced by': None,\n",
       "  'replaces': None,\n",
       "  'residence': None,\n",
       "  'screenwriter': None,\n",
       "  'separated from': None,\n",
       "  'series': None,\n",
       "  'sibling': None,\n",
       "  'sister city': None,\n",
       "  'spouse': None,\n",
       "  'start time': None,\n",
       "  'subclass of': None,\n",
       "  'subsidiary': None,\n",
       "  'territory claimed by': None,\n",
       "  'unemployment rate': None,\n",
       "  'work location': None}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['entity'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"relation_info_dict = {}\n",
    "for id, relation in enumerate(dataset[0]['relation'].keys()):\n",
    "    relation_info_dict[relation] = id\n",
    "\n",
    "with open('DocRED/data/relation-index.json', 'w') as f:\n",
    "    json.dump(relation_info_dict, f)\"\"\"\n",
    "\n",
    "with open('DocRED/data/relation-index.json') as f:\n",
    "    relation_info_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner(example, tokenizer, padding=True):\n",
    "    texts = example['text']\n",
    "    input_texts = []\n",
    "    for index in range(len(texts)):\n",
    "        # entity extraction and NER\n",
    "        text = texts[index].lower().strip() + \" [learn1] [learn2]\"\n",
    "        for entity in example['entity'][index]:\n",
    "            text = text + \" entity : \" + entity[0] + \" , type : \" + entity[1] + \" ;\"\n",
    "        text = text[:-1] + \".\"\n",
    "        # print(\"1\")\n",
    "        # add relation classificaiton\n",
    "        text = text.lower().strip() + \" [learn3] [learn4]\"\n",
    "        for relation_type, relation_pair in example['relation'][index].items():\n",
    "            if relation_pair:\n",
    "                text_w_relation = text + \" for the relation \" + relation_type + \" : 1 .\" + tokenizer.eos_token\n",
    "\n",
    "            else:\n",
    "                text_w_relation = text + \" for the relation \" + relation_type + \" : 0 .\" + tokenizer.eos_token\n",
    "            \n",
    "            input_texts.append(text_w_relation)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_texts\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [00:14<00:00, 54.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# feed the dataset:dataset to the pro_processing_ner() function with tokenizer, at each time, we feed 30 examples to the function, and then save the output to a json file\n",
    "# each time the return of the function is a dict, we need to save the dict to a list, and then save the list to a json file\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output = {\"input_texts\": []}\n",
    "\n",
    "for i in tqdm(range(0, len(dataset), 30)):\n",
    "    result = pro_processing_ner(dataset[i:i+30], tokenizer)\n",
    "    output[\"input_texts\"].extend(result[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2328576"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['input_texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the output[\"input_texts\"] into a dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "input_text_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'input_texts': output['input_texts'],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed1deaf780d4a6dbec6d87cfe829c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2328576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = input_text_dataset.map(lambda example: tokenizer(example['input_texts'], padding='max_length', truncation=True, max_length=1024, pad_to_max_length=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 2328576\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the column of input_texts in the tokenized_dataset\n",
    "tokenized_dataset.remove_columns('input_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28318b491ba841fb9dedfaf1a10b3ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/27 shards):   0%|          | 0/2328576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the tokenized_dataset\n",
    "\n",
    "tokenized_dataset.save_to_disk('DocRED/GPT_w_ner_short_relation_onlyRC/train_data_ner_short_relation_onlyRC')\n",
    "# with open('DocRED/data/train_ner_short_relation.json', 'w') as f:\n",
    "#     json.dump(tokenized_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_dataset = Dataset.load_from_disk('DocRED/GPT_w_ner_short_relation_onlyRC/train_data_ner_short_relation_onlyRC')\n",
    "tokenized_dataset = tokenized_dataset.remove_columns('input_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take the first len(tokenized_dataset) // 50 examples to train the model\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select(range(len(tokenized_dataset) // 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zest airways, inc. operated as airasia zest ( formerly asian spirit and zest air ), was a low - cost airline based at the ninoy aquino international airport in pasay city, metro manila in the philippines. it operated scheduled domestic and international tourist services, mainly feeder services linking manila and cebu with 24 domestic destinations in support of the trunk route operations of other airlines. [learn1] [learn2] entity : zest airways, inc., type : organization ; entity : airasia zest, type : organization ; entity : asian spirit and zest air, type : organization ; entity : ninoy aquino international airport, type : location ; entity : pasay city, type : location ; entity : metro manila, type : location ; entity : philippines, type : location ; entity : manila, type : location ; entity : cebu, type : location ; entity : 24, type : number. [learn3] [learn4] for the relation participant of : 0.<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset[66]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([   89,   395,  1633,  ..., 50258, 50258, 50258]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])},\n",
       " {'input_ids': tensor([   89,   395,  1633,  ..., 50258, 50258, 50258]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.__getitems__([1,4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/intermediate-sv/wandb/run-20230929_012754-019v7pcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2-intermediate/runs/019v7pcn' target=\"_blank\">GPT2-short_relation_onlyRC_DocRED-w-ner-5epochs</a></strong> to <a href='https://wandb.ai/tian1995/GPT2-intermediate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2-intermediate' target=\"_blank\">https://wandb.ai/tian1995/GPT2-intermediate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2-intermediate/runs/019v7pcn' target=\"_blank\">https://wandb.ai/tian1995/GPT2-intermediate/runs/019v7pcn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2-intermediate/runs/019v7pcn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4a44d09c60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2-intermediate\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"GPT2-short_relation_onlyRC_DocRED-w-ner-5epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4, \n",
    "        # fp16=True,\n",
    "        logging_steps=100, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='DocRED/GPT_w_ner_short_relation_onlyRC'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2693602bf51d4a89a2ecbd40fddf1651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 24.2481, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 1.1111, 'learning_rate': 4e-05, 'epoch': 0.86}\n",
      "{'loss': 0.1935, 'learning_rate': 6e-05, 'epoch': 1.29}\n",
      "{'loss': 0.1151, 'learning_rate': 8e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0929, 'learning_rate': 0.0001, 'epoch': 2.15}\n",
      "{'loss': 0.0797, 'learning_rate': 0.00012, 'epoch': 2.58}\n",
      "{'loss': 0.0785, 'learning_rate': 0.00014, 'epoch': 3.0}\n",
      "{'loss': 0.0747, 'learning_rate': 0.00016, 'epoch': 3.43}\n",
      "{'loss': 0.0629, 'learning_rate': 0.00018, 'epoch': 3.86}\n",
      "{'loss': 0.0676, 'learning_rate': 0.0002, 'epoch': 4.29}\n",
      "{'loss': 0.0568, 'learning_rate': 7.878787878787879e-05, 'epoch': 4.72}\n",
      "{'train_runtime': 996.1626, 'train_samples_per_second': 4.673, 'train_steps_per_second': 1.169, 'train_loss': 2.25011079792301, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1165, training_loss=2.25011079792301, metrics={'train_runtime': 996.1626, 'train_samples_per_second': 4.673, 'train_steps_per_second': 1.169, 'train_loss': 2.25011079792301, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█▃</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>1165</td></tr><tr><td>train/learning_rate</td><td>8e-05</td></tr><tr><td>train/loss</td><td>0.0568</td></tr><tr><td>train/total_flos</td><td>2432628817920000.0</td></tr><tr><td>train/train_loss</td><td>2.25011</td></tr><tr><td>train/train_runtime</td><td>996.1626</td></tr><tr><td>train/train_samples_per_second</td><td>4.673</td></tr><tr><td>train/train_steps_per_second</td><td>1.169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT2-short_relation_onlyRC_DocRED-w-ner-5epochs</strong> at: <a href='https://wandb.ai/tian1995/GPT2-intermediate/runs/019v7pcn' target=\"_blank\">https://wandb.ai/tian1995/GPT2-intermediate/runs/019v7pcn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230929_012754-019v7pcn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('DocRED/GPT_w_ner_short_relation_onlyRC/tokenizer/tokenizer_config.json',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/tokenizer/special_tokens_map.json',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/tokenizer/vocab.json',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/tokenizer/merges.txt',\n",
       " 'DocRED/GPT_w_ner_short_relation_onlyRC/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "trainer.save_model(\"DocRED/GPT_w_ner_short_relation_onlyRC/model\")\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"DocRED/GPT_w_ner_short_relation_onlyRC/tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"DocRED/GPT_w_ner_short_relation_onlyRC/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"DocRED/GPT_w_ner_short_relation/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|pad|>',\n",
       " '[learn1]',\n",
       " '[learn2]',\n",
       " '[learn3]',\n",
       " '[learn4]',\n",
       " '[learn5]',\n",
       " '[learn6]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output all of the special tokens in the tokenizer\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> [learn2] entity : atlantic, type : location ; entity : philippines, type : location\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "# inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. [learn1] [learn2] entity :\", return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again.\", return_tensors=\"pt\", padding='max_length', max_length=1000)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=20, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ner = 1\n",
    "\n",
    "test_relation_dict = {}\n",
    "if ner:\n",
    "    with open('DocRED/data/bi-sent-pre-process_test.json') as f:\n",
    "        test_relation_dict = json.load(f)\n",
    "\n",
    "    test_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': test_relation_dict['text'],\n",
    "            'entity': test_relation_dict['entity'],\n",
    "            'relation': test_relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:04<00:00, 61.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def pro_processing_ner(example, tokenizer, padding=True):\n",
    "    texts = example['text']\n",
    "    input_texts = []\n",
    "    for index in range(len(texts)):\n",
    "        # entity extraction and NER\n",
    "        text = texts[index].lower().strip() + \" [learn1] [learn2]\"\n",
    "        for entity in example['entity'][index]:\n",
    "            text = text + \" entity : \" + entity[0] + \" , type : \" + entity[1] + \" ;\"\n",
    "        text = text[:-1] + \".\"\n",
    "        # print(\"1\")\n",
    "        # add relation classificaiton\n",
    "        text = text.lower().strip() + \" [learn3] [learn4]\"\n",
    "        for relation_type, relation_pair in example['relation'][index].items():\n",
    "            if relation_pair:\n",
    "                text_w_relation = text + \" for the relation \" + relation_type + \" : 1 .\" + tokenizer.eos_token\n",
    "\n",
    "            else:\n",
    "                text_w_relation = text + \" for the relation \" + relation_type + \" : 0 .\" + tokenizer.eos_token\n",
    "            \n",
    "            input_texts.append(text_w_relation)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_texts\n",
    "        }\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output = {\"input_texts\": []}\n",
    "\n",
    "for i in tqdm(range(0, len(dataset), 30)):\n",
    "    result = pro_processing_ner(dataset[i:i+30], tokenizer)\n",
    "    output[\"input_texts\"].extend(result[\"input_ids\"])\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "input_text_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'input_texts': output['input_texts'],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_text_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenized_dataset \u001b[39m=\u001b[39m input_text_dataset\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m example: tokenizer(example[\u001b[39m'\u001b[39m\u001b[39minput_texts\u001b[39m\u001b[39m'\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, pad_to_max_length\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenized_dataset\u001b[39m.\u001b[39mremove_columns(\u001b[39m'\u001b[39m\u001b[39minput_texts\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenized_dataset\u001b[39m.\u001b[39msave_to_disk(\u001b[39m'\u001b[39m\u001b[39mDocRED/GPT_w_ner_short_relation_onlyRC/test_data_ner_short_relation_onlyRC\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_text_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = input_text_dataset.map(lambda example: tokenizer(example['input_texts'], padding='max_length', truncation=True, max_length=1024, pad_to_max_length=True), batched=True)\n",
    "\n",
    "tokenized_dataset.remove_columns('input_texts')\n",
    "\n",
    "tokenized_dataset.save_to_disk('DocRED/GPT_w_ner_short_relation_onlyRC/test_data_ner_short_relation_onlyRC')\n",
    "# with open('DocRED/data/train_ner_short_relation.json', 'w') as f:\n",
    "#     json.dump(tokenized_dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly select test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_test_dataset = Dataset.load_from_disk('DocRED/GPT_w_ner_short_relation_onlyRC/test_data_ner_short_relation_onlyRC')\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns('input_texts')\n",
    "\n",
    "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773472"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skai tv is a greek free - to - air television network based in piraeus. it is part of the skai group, one of the largest media groups in the country. [learn1] [learn2] entity : skai tv, type : organization ; entity : greek, type : location ; entity : piraeus, type : location ; entity : skai group, type : organization. [learn3] [learn4] for the relation applies to jurisdiction : 0.<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whole inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def for_the_relation (index, input, learn_token=False, tokenizer=tokenizer):\n",
    "    index = index % 96\n",
    "    rel_info = {}\n",
    "    with open('DocRED/data/relation-index.json') as f:\n",
    "        rel_info = json.load(f)\n",
    "    rel_info_list = [relation for relation in rel_info.keys()]\n",
    "    if learn_token:\n",
    "        relation_input = \"[learn3] [learn4] for the relation \" + rel_info_list[index].lower() + \" : \"\n",
    "    else:\n",
    "        relation_input = \"for the relation \" + rel_info_list[index].lower() + \" : \"\n",
    "    tokenized_relation_input = tokenizer.encode(relation_input, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    # print(\"before relation classification: \", tokenizer.decode(input))\n",
    "    return (torch.cat((input, tokenized_relation_input[0]), dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2974b645a2bf478d8400e2b11b70df59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/773472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_252304/789588544.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids_list['input_ids'][:end]).to(\"cuda\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for golden 1: 0.0\n",
      "accuracy for golden 0: 0.0\n",
      "total accuracy:  0.0\n",
      "accuracy for golden 1: 0.99999999995\n",
      "accuracy for golden 0: 0.010526315789462604\n",
      "total accuracy:  0.030927835051514505\n",
      "accuracy for golden 1: 0.99999999995\n",
      "accuracy for golden 0: 0.02094240837695239\n",
      "total accuracy:  0.0310880829015383\n",
      "accuracy for golden 1: 0.99999999995\n",
      "accuracy for golden 0: 0.013937282229960301\n",
      "total accuracy:  0.0207612456747333\n",
      "accuracy for golden 1: 0.9999999999666667\n",
      "accuracy for golden 0: 0.010471204188478935\n",
      "total accuracy:  0.01818181818181346\n",
      "accuracy for golden 1: 0.999999999975\n",
      "accuracy for golden 0: 0.014675052410898391\n",
      "total accuracy:  0.022869022869018114\n",
      "accuracy for golden 1: 0.99999999998\n",
      "accuracy for golden 0: 0.01398601398601154\n",
      "total accuracy:  0.02253032928942417\n",
      "accuracy for golden 1: 0.99999999998\n",
      "accuracy for golden 0: 0.011976047904189824\n",
      "total accuracy:  0.019316493313518673\n",
      "accuracy for golden 1: 0.9999999999888889\n",
      "accuracy for golden 0: 0.010526315789472298\n",
      "total accuracy:  0.022106631989594002\n",
      "accuracy for golden 1: 0.99999999999\n",
      "accuracy for golden 0: 0.009356725146197736\n",
      "total accuracy:  0.020809248554910887\n",
      "accuracy for golden 1: 0.999999999990909\n",
      "accuracy for golden 0: 0.010526315789472575\n",
      "total accuracy:  0.021852237252859327\n",
      "accuracy for golden 1: 0.999999999990909\n",
      "accuracy for golden 0: 0.009560229445505778\n",
      "total accuracy:  0.019867549668872293\n",
      "accuracy for golden 1: 0.999999999990909\n",
      "accuracy for golden 0: 0.010507880910682091\n",
      "total accuracy:  0.01994796183867997\n",
      "accuracy for golden 1: 0.9999999999933333\n",
      "accuracy for golden 0: 0.009724473257697753\n",
      "total accuracy:  0.021617293835066324\n",
      "accuracy for golden 1: 0.9999999999933333\n",
      "accuracy for golden 0: 0.010526315789472893\n",
      "total accuracy:  0.02156133828996122\n",
      "accuracy for golden 1: 0.9999999999933333\n",
      "accuracy for golden 0: 0.009817671809255973\n",
      "total accuracy:  0.02012491325468285\n",
      "accuracy for golden 1: 0.9999999999933333\n",
      "accuracy for golden 0: 0.00919842312746326\n",
      "total accuracy:  0.018867924528300658\n",
      "accuracy for golden 1: 0.9999999999937501\n",
      "accuracy for golden 0: 0.009276437847865845\n",
      "total accuracy:  0.018983466013470975\n",
      "accuracy for golden 1: 0.9999999999941177\n",
      "accuracy for golden 0: 0.008761682242990142\n",
      "total accuracy:  0.018507807981491122\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "50260 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb Cell 55\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# feed the actually_input to the model by 10 examples each time\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m input_index, input_ids_list \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(tokenized_test_dataset)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m# print(input_index + 1, \" / \", len(tokenized_test_dataset))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         end \u001b[39m=\u001b[39m input_ids_list[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist()\u001b[39m.\u001b[39;49mindex(tokenizer\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(\u001b[39m\"\u001b[39;49m\u001b[39m[learn2]\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(input_ids_list[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][:end])\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y120sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         relation_classfication \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 50260 is not in list"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "output_results = []\n",
    "\n",
    "\n",
    "right_relation_1 = []\n",
    "right_relation_0 = []\n",
    "wrong_relation_1 = []\n",
    "wrong_relation_0 = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # feed the actually_input to the model by 10 examples each time\n",
    "    for input_index, input_ids_list in enumerate(tqdm(tokenized_test_dataset)):\n",
    "        # print(input_index + 1, \" / \", len(tokenized_test_dataset))\n",
    "        end = input_ids_list['input_ids'].tolist().index(tokenizer.convert_tokens_to_ids(\"[learn2]\")) + 1\n",
    "        input_ids = torch.tensor(input_ids_list['input_ids'][:end]).to(\"cuda\")\n",
    "        relation_classfication = False\n",
    "\n",
    "        while((input_ids[-1].item() != tokenizer.eos_token_id) and (len(input_ids) < 1024)):\n",
    "            output = model(input_ids=input_ids)\n",
    "            current_output = np.array(output['logits'].cpu())\n",
    "            max_index = np.argmax(current_output[-1, :], axis=0)\n",
    "\n",
    "            # if len(input_ids) - end > 40 and (not relation_classfication):\n",
    "            #     input_ids = torch.cat((input_ids, torch.tensor(max_index).unsqueeze(0).to(\"cuda\")), dim=0)\n",
    "            #     input_ids = for_the_relation(input_index, input_ids.to(\"cpu\"), learn_token=True)\n",
    "            #     input_ids = input_ids.to(\"cuda\")\n",
    "            #     relation_classfication = True\n",
    "            #     continue\n",
    "\n",
    "            if max_index == tokenizer.convert_tokens_to_ids(\"[learn4]\") and (not relation_classfication):\n",
    "                input_ids = torch.cat((input_ids, torch.tensor(max_index).unsqueeze(0).to(\"cuda\")), dim=0)\n",
    "                input_ids = for_the_relation(input_index, input_ids.to(\"cpu\"))\n",
    "                input_ids = input_ids.to(\"cuda\")\n",
    "                relation_classfication = True\n",
    "                continue\n",
    "\n",
    "            if relation_classfication:\n",
    "\n",
    "                zero_index = tokenizer.convert_tokens_to_ids(\"0\")\n",
    "                one_index = tokenizer.convert_tokens_to_ids(\"1\")\n",
    "                zero_possibility = current_output[-1, zero_index]\n",
    "                # print(\"zero_possibility: \", zero_possibility)\n",
    "                one_possibility = current_output[-1, one_index]\n",
    "                # print(\"one_possibility: \", one_possibility)\n",
    "\n",
    "\n",
    "                if zero_possibility > one_possibility:\n",
    "                    output_results.append(0)\n",
    "                else:\n",
    "                    output_results.append(1)\n",
    "                \n",
    "                if output_results[-1]:\n",
    "                    if input_text_dataset[input_index]['input_texts'][-16] == \"1\":\n",
    "                        right_relation_1.append(input_index)\n",
    "                    else:\n",
    "                        wrong_relation_1.append(input_index)\n",
    "\n",
    "                else:\n",
    "                    if input_text_dataset[input_index]['input_texts'][-16] == \"0\":\n",
    "                        right_relation_0.append(input_index)\n",
    "                    else:\n",
    "                        wrong_relation_0.append(input_index)\n",
    "                break\n",
    "            \n",
    "            input_ids = torch.cat((input_ids, torch.tensor(max_index).unsqueeze(0).to(\"cuda\")), dim=0)\n",
    "            \n",
    "        if input_index % 96 == 0:\n",
    "            print(\"accuracy for golden 1:\", len(right_relation_1) / (len(right_relation_1) + len(wrong_relation_0) + 1e-10))\n",
    "            print(\"accuracy for golden 0:\", len(right_relation_0) / (len(right_relation_0) + len(wrong_relation_1) + 1e-10))\n",
    "            print(\"total accuracy: \", (len(right_relation_1) + len(right_relation_0)) / (len(right_relation_1) + len(right_relation_0) + len(wrong_relation_1) + len(wrong_relation_0) + 1e-10))\n",
    "\n",
    "\n",
    "        # # only take the first 96 * 100 examples to test\n",
    "        # if input_index + 1 == 96 * 100:\n",
    "        #     break\n",
    "        \n",
    "    # print(tokenizer.batch_decode(max_index, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "50260 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try-short-relation_onlyRC.ipynb#Y160sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_ids_list[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist()\u001b[39m.\u001b[39;49mindex(tokenizer\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(\u001b[39m\"\u001b[39;49m\u001b[39m[learn2]\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: 50260 is not in list"
     ]
    }
   ],
   "source": [
    "input_ids_list['input_ids'].tolist().index(tokenizer.convert_tokens_to_ids(\"[learn2]\")) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the third strategic area is nanotechnology applied to the development of the smarter devices for the intermittent production industry. this technology can be applied to, for example, blood testing or recovering oil from existing fields. [learn1] [learn2. [learn3] [learn4] for the relation applies to jurisdiction : 0.<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids_list['input_ids'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_relation_1:  17\n",
      "right_relation_0:  15\n",
      "wrong_relation_1:  1792\n",
      "wrong_relation_0:  0\n",
      "accuracy for golden 1: 1.0\n",
      "accuracy for golden 0: 0.008301051466519093\n",
      "total accuracy:  0.017543859649122806\n"
     ]
    }
   ],
   "source": [
    "print(\"right_relation_1: \", len(right_relation_1))\n",
    "print(\"right_relation_0: \", len(right_relation_0))\n",
    "print(\"wrong_relation_1: \", len(wrong_relation_1))\n",
    "print(\"wrong_relation_0: \", len(wrong_relation_0))\n",
    "print(\"accuracy for golden 1:\", len(right_relation_1) / (len(right_relation_1) + len(wrong_relation_0)))\n",
    "print(\"accuracy for golden 0:\", len(right_relation_0) / (len(right_relation_0) + len(wrong_relation_1)))\n",
    "print(\"total accuracy: \", (len(right_relation_1) + len(right_relation_0)) / (len(right_relation_1) + len(right_relation_0) + len(wrong_relation_1) + len(wrong_relation_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ner = 1\n",
    "\n",
    "test_relation_dict = {}\n",
    "if ner:\n",
    "    with open('DocRED/data/bi-sent-pre-process_test.json') as f:\n",
    "        test_relation_dict = json.load(f)\n",
    "\n",
    "    test_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': test_relation_dict['text'],\n",
    "            'entity': test_relation_dict['entity'],\n",
    "            'relation': test_relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output_texts to a json file\n",
    "\n",
    "with open(f'DocRED/GPT_w_ner_short_relation/test_output_texts_for_test_dataset_{random_test_index}.json', 'w') as f:\n",
    "    json.dump(output_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_info = {}\n",
    "with open('DocRED/data/relation-index.json') as f:\n",
    "    rel_info = json.load(f)\n",
    "rel_info_list = [relation for relation in rel_info.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'durgada is a rural village in gollaprolu mandal , east godavari district , andhra pradesh , india . the village was formerly known as durga ooda , durga vaahini .',\n",
       " 'entity': [['durgada', 'location'],\n",
       "  ['gollaprolu', 'location'],\n",
       "  ['east godavari', 'location'],\n",
       "  ['andhra pradesh', 'location'],\n",
       "  ['india', 'location'],\n",
       "  ['durga ooda', 'location'],\n",
       "  ['durga vaahini', 'location']],\n",
       " 'relation': {'applies to jurisdiction': None,\n",
       "  'author': None,\n",
       "  'award received': None,\n",
       "  'basin country': None,\n",
       "  'capital': None,\n",
       "  'capital of': None,\n",
       "  'cast member': None,\n",
       "  'chairperson': None,\n",
       "  'characters': None,\n",
       "  'child': None,\n",
       "  'composer': None,\n",
       "  'conflict': None,\n",
       "  'contains administrative territorial entity': [['india', 'andhra pradesh']],\n",
       "  'continent': None,\n",
       "  'country': [['east godavari', 'india'],\n",
       "   ['durga ooda', 'india'],\n",
       "   ['durga vaahini', 'india'],\n",
       "   ['andhra pradesh', 'india'],\n",
       "   ['gollaprolu', 'india'],\n",
       "   ['durgada', 'india']],\n",
       "  'country of citizenship': None,\n",
       "  'country of origin': None,\n",
       "  'creator': None,\n",
       "  'date of birth': None,\n",
       "  'date of death': None,\n",
       "  'developer': None,\n",
       "  'director': None,\n",
       "  'dissolved, abolished or demolished': None,\n",
       "  'educated at': None,\n",
       "  'employer': None,\n",
       "  'end time': None,\n",
       "  'ethnic group': None,\n",
       "  'father': None,\n",
       "  'followed by': None,\n",
       "  'follows': None,\n",
       "  'founded by': None,\n",
       "  'genre': None,\n",
       "  'has part': None,\n",
       "  'head of government': None,\n",
       "  'head of state': None,\n",
       "  'headquarters location': None,\n",
       "  'inception': None,\n",
       "  'influenced by': None,\n",
       "  'instance of': None,\n",
       "  'languages spoken, written or signed': None,\n",
       "  'league': None,\n",
       "  'legislative body': None,\n",
       "  'located in or next to body of water': None,\n",
       "  'located in the administrative territorial entity': [['east godavari',\n",
       "    'andhra pradesh'],\n",
       "   ['andhra pradesh', 'india'],\n",
       "   ['gollaprolu', 'east godavari'],\n",
       "   ['gollaprolu', 'andhra pradesh'],\n",
       "   ['durgada', 'gollaprolu']],\n",
       "  'located on terrain feature': None,\n",
       "  'location': None,\n",
       "  'location of formation': None,\n",
       "  'lyrics by': None,\n",
       "  'manufacturer': None,\n",
       "  'member of': None,\n",
       "  'member of political party': None,\n",
       "  'member of sports team': None,\n",
       "  'military branch': None,\n",
       "  'mother': None,\n",
       "  'mouth of the watercourse': None,\n",
       "  'narrative location': None,\n",
       "  'notable work': None,\n",
       "  'official language': None,\n",
       "  'operator': None,\n",
       "  'original language of work': None,\n",
       "  'original network': None,\n",
       "  'owned by': None,\n",
       "  'parent organization': None,\n",
       "  'parent taxon': None,\n",
       "  'part of': None,\n",
       "  'participant': None,\n",
       "  'participant of': None,\n",
       "  'performer': None,\n",
       "  'place of birth': None,\n",
       "  'place of death': None,\n",
       "  'platform': None,\n",
       "  'point in time': None,\n",
       "  'position held': None,\n",
       "  'present in work': None,\n",
       "  'producer': None,\n",
       "  'product or material produced': None,\n",
       "  'production company': None,\n",
       "  'publication date': None,\n",
       "  'publisher': None,\n",
       "  'record label': None,\n",
       "  'religion': None,\n",
       "  'replaced by': None,\n",
       "  'replaces': None,\n",
       "  'residence': None,\n",
       "  'screenwriter': None,\n",
       "  'separated from': None,\n",
       "  'series': None,\n",
       "  'sibling': None,\n",
       "  'sister city': None,\n",
       "  'spouse': None,\n",
       "  'start time': None,\n",
       "  'subclass of': None,\n",
       "  'subsidiary': None,\n",
       "  'territory claimed by': None,\n",
       "  'unemployment rate': None,\n",
       "  'work location': None}}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[random_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entity : italy, type : location ; entity : greece, type : location ; entity : st. louis, type : location ; entity : india, type : location ; entity : daimaru, type : location ; entity : cowichan lake, type : location ; entity : kahlo, type : head of government. [learn3] [learn4] for the relation applies to jurisdiction : 1. [learn5] [learn6] and the entity for the relation applies to jurisdiction are : head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india; head entity: italy, tail entity: india.<|endoftext|>'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation :  contains administrative territorial entity\n",
      "[['india', 'andhra pradesh']]\n",
      "head entity:  india\n",
      "tail entity:  kahlo\n",
      "relation :  country\n",
      "[['east godavari', 'india'], ['durga ooda', 'india'], ['durga vaahini', 'india'], ['andhra pradesh', 'india'], ['gollaprolu', 'india'], ['durgada', 'india']]\n",
      "head entity:  bharatiya\n",
      "tail entity:  india\n",
      "relation :  located in the administrative territorial entity\n",
      "[['east godavari', 'andhra pradesh'], ['andhra pradesh', 'india'], ['gollaprolu', 'east godavari'], ['gollaprolu', 'andhra pradesh'], ['durgada', 'gollaprolu']]\n",
      "head entity:  greece\n",
      "tail entity:  india\n"
     ]
    }
   ],
   "source": [
    "right_relation_1 = []\n",
    "right_relation_0 = []\n",
    "wrong_relation_1 = []\n",
    "wrong_relation_0 = []\n",
    "entity_pairs = []\n",
    "\n",
    "# text = output_texts[0]\n",
    "\n",
    "# rel_index = 0\n",
    "\n",
    "for rel_index in range(len(rel_info_list)):\n",
    "    text = output_texts[rel_index]\n",
    "    relation_text = \"for the relation \" + rel_info_list[rel_index].lower() + \" : \"\n",
    "    relation_classfication = text.split(relation_text)[1].split(\".\")[0].strip()\n",
    "\n",
    "    if \"1\" in relation_classfication:\n",
    "        if test_dataset[random_test_index]['relation'][rel_info_list[rel_index]]:\n",
    "            print(\"relation : \", rel_info_list[rel_index])\n",
    "            print(\"gold truth: \", test_dataset[random_test_index]['relation'][rel_info_list[rel_index]])\n",
    "            right_relation_1.append(rel_index)\n",
    "\n",
    "            entity_pair_for_this_relation = []\n",
    "            remain_text = text\n",
    "            while(\"tail entity\" in remain_text):\n",
    "                if \";\" in remain_text.split(\"tail entity\")[1]:\n",
    "                    head_entity = remain_text.split(\"head entity: \")[1].split(\", tail entity: \")[0].strip()\n",
    "                    tail_entity = remain_text.split(\"tail entity: \")[1].split(\";\")[0].strip()\n",
    "                    if [head_entity, tail_entity] not in entity_pair_for_this_relation:\n",
    "                        entity_pair_for_this_relation.append([head_entity, tail_entity])\n",
    "                        print(\"head entity: \", head_entity)\n",
    "                        print(\"tail entity: \", tail_entity)\n",
    "                    remain_text = remain_text.split(\";\")[1].strip()\n",
    "                else:\n",
    "                    head_entity = remain_text.split(\"head entity: \")[1].split(\", tail entity: \")[0].strip()\n",
    "                    tail_entity = remain_text.split(\"tail entity: \")[1].split(\".<|endoftext|>\")[0].strip()\n",
    "                    if [head_entity, tail_entity] not in entity_pair_for_this_relation:\n",
    "                        entity_pair_for_this_relation.append([head_entity, tail_entity])\n",
    "                        print(\"head entity: \", head_entity)\n",
    "                        print(\"tail entity: \", tail_entity)\n",
    "                    break\n",
    "            entity_pairs.append(entity_pair_for_this_relation)\n",
    "\n",
    "        else:\n",
    "            wrong_relation_1.append(rel_index)\n",
    "    else:\n",
    "        if test_dataset[random_test_index]['relation'][rel_info_list[rel_index]]:\n",
    "            wrong_relation_0.append(rel_index)\n",
    "        else:\n",
    "            right_relation_0.append(rel_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_relation_1:  3\n",
      "right_relation_0:  4\n",
      "wrong_relation_1:  89\n",
      "wrong_relation_0:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"right_relation_1: \", len(right_relation_1))\n",
    "print(\"right_relation_0: \", len(right_relation_0))\n",
    "print(\"wrong_relation_1: \", len(wrong_relation_1))\n",
    "print(\"wrong_relation_0: \", len(wrong_relation_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['india', 'kahlo']], [['bharatiya', 'india']], [['greece', 'india']]]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "play ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50263"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"[learn5]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'durgada is a rural village in gollaprolu mandal, east godavari district, andhra pradesh, india. the village was formerly known as durga ooda, durga vaahini. [learn1] [learn2] entity : durgada, type : location ; entity : gollaprolu, type : location ; entity : east godavari, type : location ; entity : andhra pradesh, type : location ; entity : india, type : location ; entity : durga ooda, type : location ; entity : durga vaahini, type : location. [learn3] [learn4] for the relation contains administrative territorial entity :'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the index of 50262 in input_ids_lists[12]\n",
    "\n",
    "tokenizer.decode(input_ids_lists[12][:input_ids_lists[12].index(50263) - 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids_lists[12][:input_ids_lists[12].index(50263) - 2]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atar\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids)\n",
    "    current_output = np.array(output['logits'].cpu())\n",
    "    max_index = np.argmax(current_output[-1, :], axis=0)\n",
    "    sorted_index = np.argsort(current_output[-1, :])[::-1]\n",
    "    print(tokenizer.decode(max_index))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ito'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sorted_index[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "durgada is a rural village in gollaprolu mandal, east godavari district, andhra pradesh, india. the village was formerly known as durga ooda, durga vaahini. [learn1] [learn2] entity : durgada, type : location ; entity : gollaprolu, type : location ; entity : east godavari, type : location ; entity : andhra pradesh, type : location ; entity : india, type : location ; entity : durga ooda, type : location ; entity : durga vaahini, type : location. [learn3] [learn4] for the relation contains administrative territorial entity : 1. [learn5] [learn6] and the entity for the relation contains administrative administrative territorial entity are : head entity: india, tail entity: godav\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = torch.cat((input_ids, torch.tensor(sorted_index [0]).unsqueeze(0).to(\"cuda\")), dim=0)\n",
    "    print(tokenizer.decode(input_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
