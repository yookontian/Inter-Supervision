{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = {'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "\n",
    "# configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3aa2345a9448f2b1c6f6822dbaad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc57df67e84d5180355a279c63e95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd3a492ecf44420824c6a6d1dd4ec60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6823da69cbdb44faa60db6e2504a6418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00cf0aa98ea46c9acbe14b4ce1901c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('gpt2-medium', output_hidden_states=False)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 124439808 || all params: 124439808 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 354823168 || all params: 354823168 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DocRED/GPT_w_ner[]/gpt2_tokenizer/tokenizer_config.json',\n",
       " 'DocRED/GPT_w_ner[]/gpt2_tokenizer/special_tokens_map.json',\n",
       " 'DocRED/GPT_w_ner[]/gpt2_tokenizer/vocab.json',\n",
       " 'DocRED/GPT_w_ner[]/gpt2_tokenizer/merges.txt',\n",
       " 'DocRED/GPT_w_ner[]/gpt2_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "\n",
    "# tokenizer.save_pretrained('DocRED/GPT_w_ner[]/gpt2_tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/train_annotated.json') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vertexSet': [[{'pos': [0, 4],\n",
       "    'type': 'ORG',\n",
       "    'sent_id': 0,\n",
       "    'name': 'Zest Airways, Inc.'},\n",
       "   {'sent_id': 0,\n",
       "    'type': 'ORG',\n",
       "    'pos': [10, 15],\n",
       "    'name': 'Asian Spirit and Zest Air'},\n",
       "   {'name': 'AirAsia Zest', 'pos': [6, 8], 'sent_id': 0, 'type': 'ORG'},\n",
       "   {'name': 'AirAsia Zest', 'pos': [19, 21], 'sent_id': 6, 'type': 'ORG'}],\n",
       "  [{'name': 'Ninoy Aquino International Airport',\n",
       "    'pos': [4, 8],\n",
       "    'sent_id': 3,\n",
       "    'type': 'LOC'},\n",
       "   {'name': 'Ninoy Aquino International Airport',\n",
       "    'pos': [26, 30],\n",
       "    'sent_id': 0,\n",
       "    'type': 'LOC'}],\n",
       "  [{'name': 'Pasay City', 'pos': [31, 33], 'sent_id': 0, 'type': 'LOC'}],\n",
       "  [{'name': 'Metro Manila', 'pos': [34, 36], 'sent_id': 0, 'type': 'LOC'}],\n",
       "  [{'name': 'Philippines', 'pos': [38, 39], 'sent_id': 0, 'type': 'LOC'},\n",
       "   {'name': 'Philippines', 'pos': [13, 14], 'sent_id': 4, 'type': 'LOC'},\n",
       "   {'sent_id': 5,\n",
       "    'type': 'LOC',\n",
       "    'pos': [25, 29],\n",
       "    'name': 'Republic of the Philippines'}],\n",
       "  [{'name': 'Manila', 'pos': [13, 14], 'sent_id': 1, 'type': 'LOC'},\n",
       "   {'name': 'Manila', 'pos': [9, 10], 'sent_id': 3, 'type': 'LOC'}],\n",
       "  [{'name': 'Cebu', 'pos': [15, 16], 'sent_id': 1, 'type': 'LOC'}],\n",
       "  [{'pos': [17, 18], 'type': 'NUM', 'sent_id': 1, 'name': '24'}],\n",
       "  [{'pos': [1, 2], 'type': 'TIME', 'sent_id': 2, 'name': '2013'},\n",
       "   {'pos': [1, 5], 'type': 'TIME', 'sent_id': 5, 'name': 'August 16, 2013'}],\n",
       "  [{'pos': [9, 11],\n",
       "    'type': 'ORG',\n",
       "    'name': 'Philippines AirAsia',\n",
       "    'sent_id': 2}],\n",
       "  [{'pos': [5, 7], 'type': 'ORG', 'sent_id': 4, 'name': 'Asian Spirit'}],\n",
       "  [{'pos': [7, 13],\n",
       "    'type': 'ORG',\n",
       "    'sent_id': 5,\n",
       "    'name': 'Civil Aviation Authority of the Philippines'},\n",
       "   {'name': 'CAAP', 'pos': [14, 15], 'sent_id': 5, 'type': 'ORG'}],\n",
       "  [{'name': 'Zest Air', 'pos': [34, 36], 'sent_id': 5, 'type': 'ORG'},\n",
       "   {'pos': [7, 9], 'type': 'ORG', 'sent_id': 6, 'name': 'Zest Air'}],\n",
       "  [{'sent_id': 6, 'type': 'NUM', 'pos': [2, 4], 'name': 'a year'}],\n",
       "  [{'name': 'AirAsia', 'pos': [5, 6], 'sent_id': 6, 'type': 'ORG'}],\n",
       "  [{'pos': [5, 7],\n",
       "    'type': 'ORG',\n",
       "    'name': 'AirAsia Philippines',\n",
       "    'sent_id': 7}],\n",
       "  [{'pos': [8, 10], 'type': 'TIME', 'sent_id': 7, 'name': 'January 2016'}]],\n",
       " 'labels': [{'r': 'P159', 'h': 0, 't': 2, 'evidence': [0]},\n",
       "  {'r': 'P17', 'h': 0, 't': 4, 'evidence': [2, 4, 7]},\n",
       "  {'r': 'P17', 'h': 12, 't': 4, 'evidence': [6, 7]},\n",
       "  {'r': 'P17', 'h': 2, 't': 4, 'evidence': [0]},\n",
       "  {'r': 'P131', 'h': 2, 't': 3, 'evidence': [0]},\n",
       "  {'r': 'P150', 'h': 4, 't': 3, 'evidence': [0]},\n",
       "  {'r': 'P17', 'h': 5, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P150', 'h': 3, 't': 2, 'evidence': [0]},\n",
       "  {'r': 'P131', 'h': 3, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P17', 'h': 3, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P131', 'h': 1, 't': 2, 'evidence': [0, 3]},\n",
       "  {'r': 'P17', 'h': 1, 't': 4, 'evidence': [0, 3]},\n",
       "  {'r': 'P17', 'h': 10, 't': 4, 'evidence': [4]}],\n",
       " 'title': 'AirAsia Zest',\n",
       " 'sents': [['Zest',\n",
       "   'Airways',\n",
       "   ',',\n",
       "   'Inc.',\n",
       "   'operated',\n",
       "   'as',\n",
       "   'AirAsia',\n",
       "   'Zest',\n",
       "   '(',\n",
       "   'formerly',\n",
       "   'Asian',\n",
       "   'Spirit',\n",
       "   'and',\n",
       "   'Zest',\n",
       "   'Air',\n",
       "   ')',\n",
       "   ',',\n",
       "   'was',\n",
       "   'a',\n",
       "   'low',\n",
       "   '-',\n",
       "   'cost',\n",
       "   'airline',\n",
       "   'based',\n",
       "   'at',\n",
       "   'the',\n",
       "   'Ninoy',\n",
       "   'Aquino',\n",
       "   'International',\n",
       "   'Airport',\n",
       "   'in',\n",
       "   'Pasay',\n",
       "   'City',\n",
       "   ',',\n",
       "   'Metro',\n",
       "   'Manila',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   '.'],\n",
       "  ['It',\n",
       "   'operated',\n",
       "   'scheduled',\n",
       "   'domestic',\n",
       "   'and',\n",
       "   'international',\n",
       "   'tourist',\n",
       "   'services',\n",
       "   ',',\n",
       "   'mainly',\n",
       "   'feeder',\n",
       "   'services',\n",
       "   'linking',\n",
       "   'Manila',\n",
       "   'and',\n",
       "   'Cebu',\n",
       "   'with',\n",
       "   '24',\n",
       "   'domestic',\n",
       "   'destinations',\n",
       "   'in',\n",
       "   'support',\n",
       "   'of',\n",
       "   'the',\n",
       "   'trunk',\n",
       "   'route',\n",
       "   'operations',\n",
       "   'of',\n",
       "   'other',\n",
       "   'airlines',\n",
       "   '.'],\n",
       "  ['In',\n",
       "   '2013',\n",
       "   ',',\n",
       "   'the',\n",
       "   'airline',\n",
       "   'became',\n",
       "   'an',\n",
       "   'affiliate',\n",
       "   'of',\n",
       "   'Philippines',\n",
       "   'AirAsia',\n",
       "   'operating',\n",
       "   'their',\n",
       "   'brand',\n",
       "   'separately',\n",
       "   '.'],\n",
       "  ['Its',\n",
       "   'main',\n",
       "   'base',\n",
       "   'was',\n",
       "   'Ninoy',\n",
       "   'Aquino',\n",
       "   'International',\n",
       "   'Airport',\n",
       "   ',',\n",
       "   'Manila',\n",
       "   '.'],\n",
       "  ['The',\n",
       "   'airline',\n",
       "   'was',\n",
       "   'founded',\n",
       "   'as',\n",
       "   'Asian',\n",
       "   'Spirit',\n",
       "   ',',\n",
       "   'the',\n",
       "   'first',\n",
       "   'airline',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   'to',\n",
       "   'be',\n",
       "   'run',\n",
       "   'as',\n",
       "   'a',\n",
       "   'cooperative',\n",
       "   '.'],\n",
       "  ['On',\n",
       "   'August',\n",
       "   '16',\n",
       "   ',',\n",
       "   '2013',\n",
       "   ',',\n",
       "   'the',\n",
       "   'Civil',\n",
       "   'Aviation',\n",
       "   'Authority',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   '(',\n",
       "   'CAAP',\n",
       "   ')',\n",
       "   ',',\n",
       "   'the',\n",
       "   'regulating',\n",
       "   'body',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Government',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Republic',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Philippines',\n",
       "   'for',\n",
       "   'civil',\n",
       "   'aviation',\n",
       "   ',',\n",
       "   'suspended',\n",
       "   'Zest',\n",
       "   'Air',\n",
       "   'flights',\n",
       "   'until',\n",
       "   'further',\n",
       "   'notice',\n",
       "   'because',\n",
       "   'of',\n",
       "   'safety',\n",
       "   'issues',\n",
       "   '.'],\n",
       "  ['Less',\n",
       "   'than',\n",
       "   'a',\n",
       "   'year',\n",
       "   'after',\n",
       "   'AirAsia',\n",
       "   'and',\n",
       "   'Zest',\n",
       "   'Air',\n",
       "   \"'s\",\n",
       "   'strategic',\n",
       "   'alliance',\n",
       "   ',',\n",
       "   'the',\n",
       "   'airline',\n",
       "   'has',\n",
       "   'been',\n",
       "   'rebranded',\n",
       "   'as',\n",
       "   'AirAsia',\n",
       "   'Zest',\n",
       "   '.'],\n",
       "  ['The',\n",
       "   'airline',\n",
       "   'was',\n",
       "   'merged',\n",
       "   'into',\n",
       "   'AirAsia',\n",
       "   'Philippines',\n",
       "   'in',\n",
       "   'January',\n",
       "   '2016',\n",
       "   '.']]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthe names of vertextSet can be the same, but the pos should be different\\nstructure:\\n'vertexSet': \\n    [\\n        {\\n            'pos':[start, end],\\n            'type': 'NER',\\n            'sent_id': 0,\\n            'name': 'string',\\n        },\\n        {}\\n    ]\\n'labels':\\n    [\\n        {\\n            'r': 'Pxx',\\n            'h': 0,\\n            't': 1,\\n            'evidence': [2, 3, 4],\\n        },\\n        {}\\n    ]\\n'title': 'string',\\n'sents':\\n    [\\n        ['word0', 'word1',]\\n        ['word0', 'word1',]\\n    ]\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "the names of vertextSet can be the same, but the pos should be different\n",
    "structure:\n",
    "'vertexSet': \n",
    "    [\n",
    "        (for the same entity but in different synonyms and different sentences)\n",
    "        [\n",
    "            {\n",
    "                'pos':[start, end],\n",
    "                'type': 'NER',\n",
    "                'sent_id': 0,\n",
    "                'name': 'string',\n",
    "            },\n",
    "            {}\n",
    "        ],\n",
    "        [entity-2]\n",
    "    ]\n",
    "'labels':\n",
    "    [\n",
    "        {\n",
    "            'r': 'Pxx',\n",
    "            'h': 0,\n",
    "            't': 1,\n",
    "            'evidence': [2, 3, 4],\n",
    "        },\n",
    "        {}\n",
    "    ]\n",
    "'title': 'string',\n",
    "'sents':\n",
    "    [\n",
    "        ['word0', 'word1',]\n",
    "        ['word0', 'word1',]\n",
    "    ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DocRED/data/ner_info.json') as f:\n",
    "    ner_info = json.load(f)\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    relation_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# doc-level is too long for gpt-2, so we need to split the doc-level into bi-sent-level\n",
    "\n",
    "relation_dict = {\n",
    "    'id': [],\n",
    "    'text':[],\n",
    "    'entity': [],\n",
    "    'relation': []\n",
    "}\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    # id\n",
    "    relation_dict['id'].append(i)\n",
    "\n",
    "    # text\n",
    "    sents = \"\"\n",
    "    for sent in train_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # if there are space, delete the first and last space of the sents\n",
    "    sents = sents.strip()\n",
    "    # delete double space in the sents\n",
    "    sents = sents.replace(\"  \", \" \")\n",
    "    relation_dict['text'].append(sents)\n",
    "    del sents\n",
    "\n",
    "    # entity\n",
    "    entity = []\n",
    "    entity_list = []\n",
    "    entity_flat = {}\n",
    "    entity_count = 0\n",
    "    for sent_item in train_set[i]['vertexSet']:\n",
    "        for item in sent_item:\n",
    "            entity_item = []\n",
    "            if item['name'].lower() not in entity_list:\n",
    "                entity_list.append(item['name'].lower().strip())\n",
    "                entity_item.append(item['name'].lower().strip())\n",
    "                entity_item.append(ner_info[item['type']])\n",
    "\n",
    "                entity.append(entity_item)\n",
    "            \n",
    "            # add the entity_flat\n",
    "            entity_flat[entity_count] = item['name'].lower().strip()\n",
    "            entity_count += 1\n",
    "\n",
    "    # release the entity_list and entity_item\n",
    "    del entity_item\n",
    "    del entity_count\n",
    "        \n",
    "\n",
    "    # relation pairs\n",
    "    relation_pairs = {}\n",
    "    for relation_item in train_set[i]['labels']:\n",
    "        pair = []\n",
    "        head = entity_flat[relation_item['h']]\n",
    "        tail = entity_flat[relation_item['t']]\n",
    "        pair.append(head)\n",
    "        pair.append(tail)\n",
    "\n",
    "        relation  = relation_info[relation_item['r']]\n",
    "        if relation not in relation_pairs.keys():\n",
    "            relation_pairs[relation] = []\n",
    "\n",
    "        relation_pairs[relation].append(pair)\n",
    "    del pair\n",
    "    del head\n",
    "    del tail\n",
    "\n",
    "    # add the entity and relation pairs to the relation_dict\n",
    "    relation_dict['entity'].append(entity)\n",
    "    relation_dict['relation'].append(relation_pairs)\n",
    "    break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "# with open('DocRED/data/DocRED_baseline_metadata/relation_dict.json', 'w') as f:\n",
    "#     json.dump(relation_dict, f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {\n",
    "    'text':[],\n",
    "    'entity': [],\n",
    "    'relation': []\n",
    "}\n",
    "id_count = 0\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    \n",
    "\n",
    "    # text\n",
    "    sent_pairs = []\n",
    "    for sent_index, sent in enumerate(train_set[i]['sents']):\n",
    "        sents = \"\"\n",
    "\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "\n",
    "        # and the next_sent if it exists\n",
    "        try:\n",
    "            next_sent = train_set[i]['sents'][sent_index + 1]\n",
    "            b = \" \".join(next_sent)\n",
    "            sents += b.lower() + \" \"\n",
    "        except:\n",
    "            pass\n",
    "        # post process the sents for some spaces\n",
    "        sents = sents.strip()\n",
    "        sents = sents.replace(\"  \", \" \")\n",
    "\n",
    "        relation_dict['text'].append(sents)\n",
    "            \n",
    "    del sents\n",
    "\n",
    "\n",
    "    # entity\n",
    "    entity = []\n",
    "    for index in range(len(train_set[i]['sents'])):\n",
    "        # focus on the current sent and the next sent if it exists\n",
    "        if index + 1 < len(train_set[i]['sents']):\n",
    "            next_index = index + 1\n",
    "        else:\n",
    "            next_index = index\n",
    "        # group the entities for every 2 sents, no repeated entities in one group\n",
    "        c_sent_entity_lists = []\n",
    "        next_sent_entity_lists = []\n",
    "        entity_for_each_2_sents = []\n",
    "\n",
    "        for entity_spans in train_set[i]['vertexSet']:\n",
    "            for item in entity_spans:\n",
    "                entity_item = []\n",
    "                # if neither in the current sent nor in the next sent, continue\n",
    "                if item['sent_id'] != index and item['sent_id'] != next_index:\n",
    "                    continue\n",
    "                # also store the first pos of the entity in the entity_item\n",
    "                # it will look like this: [[entity_name, sent_index, pos1, ner_type]]\n",
    "                entity_item = [item['name'].lower().strip(), item['sent_id'], item['pos'][0], ner_info[item['type']]]\n",
    "\n",
    "                if entity_item[1] == index:\n",
    "                    c_sent_entity_lists.append(entity_item)\n",
    "                else:\n",
    "                    next_sent_entity_lists.append(entity_item)\n",
    "\n",
    "        # sort the c_sent_entity_lists and next_sent_entity_lists by the pos in ascending order\n",
    "        c_sent_entity_lists.sort(key=lambda x: x[2])\n",
    "        if index != next_index:\n",
    "            next_sent_entity_lists.sort(key=lambda x: x[2])\n",
    "        \n",
    "        entity_list = []\n",
    "        for item in c_sent_entity_lists:\n",
    "            if item[0] not in entity_list:\n",
    "                entity_list.append(item[0])\n",
    "                entity_for_each_2_sents.append([item[0], item[3]])\n",
    "            \n",
    "        if index != next_index:\n",
    "            for item in next_sent_entity_lists:\n",
    "                if item[0] not in entity_list:\n",
    "                    entity_list.append(item[0])\n",
    "                    entity_for_each_2_sents.append([item[0], item[3]])\n",
    "\n",
    "        relation_dict['entity'].append(entity_for_each_2_sents)\n",
    "\n",
    "    del entity_item\n",
    "    del c_sent_entity_lists\n",
    "    del next_sent_entity_lists\n",
    "    del entity_for_each_2_sents\n",
    "        \n",
    "\n",
    "    # relation pairs\n",
    "    relation_pairs = []\n",
    "\n",
    "    for index in range(len(train_set[i]['sents'])):\n",
    "        relation_pairs_for_each_2_sents = {}\n",
    "        # focus on the current sent and the next sent if it exists\n",
    "        if index + 1 < len(train_set[i]['vertexSet']):\n",
    "            next_index = index + 1\n",
    "        else:\n",
    "            next_index = index\n",
    "\n",
    "        # heads, tails: ['entity_name', start_pos]\n",
    "        for relation_item in train_set[i]['labels']:\n",
    "            heads = []\n",
    "            tails = []\n",
    "            \n",
    "            # head\n",
    "            head_exist = False\n",
    "            for head_span in train_set[i]['vertexSet'][relation_item['h']]:\n",
    "                if head_span['sent_id'] == index or head_span['sent_id'] == next_index:\n",
    "                    heads.append([head_span['name'].lower().strip(), head_span['pos'][0]])\n",
    "                    head_exist = True\n",
    "            if not head_exist:\n",
    "                continue\n",
    "    \n",
    "            # tail\n",
    "            tail_exist = False\n",
    "            for tail_span in train_set[i]['vertexSet'][relation_item['t']]:\n",
    "                if tail_span['sent_id'] == index or tail_span['sent_id'] == next_index:\n",
    "                    tails.append([tail_span['name'].lower().strip(), tail_span['pos'][0]])\n",
    "                    tail_exist = True\n",
    "            if not tail_exist:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            if relation_info[relation_item['r']] not in relation_pairs_for_each_2_sents.keys():\n",
    "                relation_pairs_for_each_2_sents[relation_info[relation_item['r']]] = []\n",
    "            for head in heads:\n",
    "                for tail in tails:\n",
    "                    relation_pairs_for_each_2_sents[relation_info[relation_item['r']]].append([head[0], tail[0]])\n",
    "\n",
    "        relation_dict['relation'].append(relation_pairs_for_each_2_sents)\n",
    "\n",
    "\n",
    "    \n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "# with open('DocRED/data/bi-sent-pre-process.json', 'w') as f:\n",
    "    # json.dump(relation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_dict['text']) == len(relation_dict['entity']) == len(relation_dict['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headquarters location': [['zest airways, inc.', 'pasay city'],\n",
       "  ['asian spirit and zest air', 'pasay city'],\n",
       "  ['airasia zest', 'pasay city']],\n",
       " 'country': [['zest airways, inc.', 'philippines'],\n",
       "  ['asian spirit and zest air', 'philippines'],\n",
       "  ['airasia zest', 'philippines'],\n",
       "  ['pasay city', 'philippines'],\n",
       "  ['manila', 'philippines'],\n",
       "  ['metro manila', 'philippines'],\n",
       "  ['ninoy aquino international airport', 'philippines']],\n",
       " 'located in the administrative territorial entity': [['pasay city',\n",
       "   'metro manila'],\n",
       "  ['metro manila', 'philippines'],\n",
       "  ['ninoy aquino international airport', 'pasay city']],\n",
       " 'contains administrative territorial entity': [['philippines',\n",
       "   'metro manila'],\n",
       "  ['metro manila', 'pasay city']]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict['relation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tian/Projects/intermediate-sv/try.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m relation_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m ner:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDocRED/data/bi-sent-pre-process.json\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         relation_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "relation_dict = {}\n",
    "if ner:\n",
    "    with open('DocRED/data/bi-sent-pre-process.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'entity': relation_dict['entity'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'entity', 'relation'],\n",
       "    num_rows: 24256\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'zest airways , inc. operated as airasia zest ( formerly asian spirit and zest air ) , was a low - cost airline based at the ninoy aquino international airport in pasay city , metro manila in the philippines . it operated scheduled domestic and international tourist services , mainly feeder services linking manila and cebu with 24 domestic destinations in support of the trunk route operations of other airlines .',\n",
       " 'entity': [['zest airways, inc.', 'organization'],\n",
       "  ['airasia zest', 'organization'],\n",
       "  ['asian spirit and zest air', 'organization'],\n",
       "  ['ninoy aquino international airport', 'location'],\n",
       "  ['pasay city', 'location'],\n",
       "  ['metro manila', 'location'],\n",
       "  ['philippines', 'location'],\n",
       "  ['manila', 'location'],\n",
       "  ['cebu', 'location'],\n",
       "  ['24', 'number']],\n",
       " 'relation': {'applies to jurisdiction': None,\n",
       "  'author': None,\n",
       "  'award received': None,\n",
       "  'basin country': None,\n",
       "  'capital': None,\n",
       "  'capital of': None,\n",
       "  'cast member': None,\n",
       "  'chairperson': None,\n",
       "  'characters': None,\n",
       "  'child': None,\n",
       "  'composer': None,\n",
       "  'conflict': None,\n",
       "  'contains administrative territorial entity': [['philippines',\n",
       "    'metro manila'],\n",
       "   ['metro manila', 'pasay city']],\n",
       "  'continent': None,\n",
       "  'country': [['zest airways, inc.', 'philippines'],\n",
       "   ['asian spirit and zest air', 'philippines'],\n",
       "   ['airasia zest', 'philippines'],\n",
       "   ['pasay city', 'philippines'],\n",
       "   ['manila', 'philippines'],\n",
       "   ['metro manila', 'philippines'],\n",
       "   ['ninoy aquino international airport', 'philippines']],\n",
       "  'country of citizenship': None,\n",
       "  'country of origin': None,\n",
       "  'creator': None,\n",
       "  'date of birth': None,\n",
       "  'date of death': None,\n",
       "  'developer': None,\n",
       "  'director': None,\n",
       "  'dissolved, abolished or demolished': None,\n",
       "  'educated at': None,\n",
       "  'employer': None,\n",
       "  'end time': None,\n",
       "  'ethnic group': None,\n",
       "  'father': None,\n",
       "  'followed by': None,\n",
       "  'follows': None,\n",
       "  'founded by': None,\n",
       "  'genre': None,\n",
       "  'has part': None,\n",
       "  'head of government': None,\n",
       "  'head of state': None,\n",
       "  'headquarters location': [['zest airways, inc.', 'pasay city'],\n",
       "   ['asian spirit and zest air', 'pasay city'],\n",
       "   ['airasia zest', 'pasay city']],\n",
       "  'inception': None,\n",
       "  'influenced by': None,\n",
       "  'instance of': None,\n",
       "  'languages spoken, written or signed': None,\n",
       "  'league': None,\n",
       "  'legislative body': None,\n",
       "  'located in or next to body of water': None,\n",
       "  'located in the administrative territorial entity': [['pasay city',\n",
       "    'metro manila'],\n",
       "   ['metro manila', 'philippines'],\n",
       "   ['ninoy aquino international airport', 'pasay city']],\n",
       "  'located on terrain feature': None,\n",
       "  'location': None,\n",
       "  'location of formation': None,\n",
       "  'lyrics by': None,\n",
       "  'manufacturer': None,\n",
       "  'member of': None,\n",
       "  'member of political party': None,\n",
       "  'member of sports team': None,\n",
       "  'military branch': None,\n",
       "  'mother': None,\n",
       "  'mouth of the watercourse': None,\n",
       "  'narrative location': None,\n",
       "  'notable work': None,\n",
       "  'official language': None,\n",
       "  'operator': None,\n",
       "  'original language of work': None,\n",
       "  'original network': None,\n",
       "  'owned by': None,\n",
       "  'parent organization': None,\n",
       "  'parent taxon': None,\n",
       "  'part of': None,\n",
       "  'participant': None,\n",
       "  'participant of': None,\n",
       "  'performer': None,\n",
       "  'place of birth': None,\n",
       "  'place of death': None,\n",
       "  'platform': None,\n",
       "  'point in time': None,\n",
       "  'position held': None,\n",
       "  'present in work': None,\n",
       "  'producer': None,\n",
       "  'product or material produced': None,\n",
       "  'production company': None,\n",
       "  'publication date': None,\n",
       "  'publisher': None,\n",
       "  'record label': None,\n",
       "  'religion': None,\n",
       "  'replaced by': None,\n",
       "  'replaces': None,\n",
       "  'residence': None,\n",
       "  'screenwriter': None,\n",
       "  'separated from': None,\n",
       "  'series': None,\n",
       "  'sibling': None,\n",
       "  'sister city': None,\n",
       "  'spouse': None,\n",
       "  'start time': None,\n",
       "  'subclass of': None,\n",
       "  'subsidiary': None,\n",
       "  'territory claimed by': None,\n",
       "  'unemployment rate': None,\n",
       "  'work location': None}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['entity'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"relation_info_dict = {}\n",
    "for id, relation in enumerate(dataset[0]['relation'].keys()):\n",
    "    relation_info_dict[relation] = id\n",
    "\n",
    "with open('DocRED/data/relation-index.json', 'w') as f:\n",
    "    json.dump(relation_info_dict, f)\"\"\"\n",
    "\n",
    "with open('DocRED/data/relation-index.json') as f:\n",
    "    relation_info_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner(example, tokenizer, padding=True):\n",
    "    texts = example['text']\n",
    "\n",
    "    for index in range(len(texts)):\n",
    "        # entity extraction and NER\n",
    "        texts[index] = texts[index].lower().strip() + \" [learn1] [learn2] \"\n",
    "        for entity in example['entity'][index]:\n",
    "            texts[index] = texts[index] + \" entity : \" + entity[0] + \" , type : \" + entity[1] + \" ;\"\n",
    "        texts[index] = texts[index][:-1] + \".\"\n",
    "        # print(\"1\")\n",
    "        # add relation classificaiton\n",
    "        texts[index] = texts[index].lower().strip() + \" [learn3] [learn4]\"\n",
    "        for relation_type, relation_pair in example['relation'][index].items():\n",
    "            if relation_pair:\n",
    "                texts[index] = texts[index] + \" relation \" + str(relation_info_dict[relation_type]) + \" : 1 ;\"\n",
    "            else:\n",
    "                texts[index] = texts[index] + \" relation \" + str(relation_info_dict[relation_type]) + \" : 0 ;\"\n",
    "\n",
    "        texts[index] = texts[index][:-1] + \".\"\n",
    "\n",
    "        # add relation extraction\n",
    "        texts[index] = texts[index].lower().strip() + \" [learn5] [learn6]\"\n",
    "        # print(\"2\")\n",
    "        for relation_type, relation_pair in example['relation'][index].items():\n",
    "            if relation_pair:\n",
    "                # print(\"text: \", texts[index])\n",
    "                # print(\"relation_pair: \", relation_pair)\n",
    "                texts[index] = texts[index] + \" for relation \" + str(relation_info_dict[relation_type]) + \" ,\"\n",
    "                for pair in relation_pair:\n",
    "                    texts[index] = texts[index] + \" head : \" + pair[0] + \" , tail : \" + pair[1] + \";\"\n",
    "                texts[index] = texts[index][:-1] + \".\"\n",
    "                \n",
    "        if texts[index][-2:] != \"6]\":\n",
    "            texts[index] = texts[index][:-1] + \". \" + tokenizer.eos_token\n",
    "        else:\n",
    "            texts[index] = texts[index] + \". \" + tokenizer.eos_token\n",
    "\n",
    "        # print(\"text: \", texts[index])\n",
    "    # print(\"3\")\n",
    "    # print(texts[0])\n",
    "    output_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "    # print(\"4\")\n",
    "    count = 0\n",
    "    for i in range(len(output_ids)):\n",
    "        ids = output_ids[i]\n",
    "        if len(ids) > 1024:\n",
    "            output_ids[i] = output_ids[i][:1023] + [tokenizer.eos_token_id]\n",
    "            count += 1\n",
    "        assert len(output_ids[i]) <= 1024\n",
    "        attention_mask.append([1] * len(output_ids[i]) + [0] * (1024 - len(output_ids[i])))\n",
    "        assert len(attention_mask[i]) == 1024\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    # print(\"5\")\n",
    "    if padding:\n",
    "        for i in range(len(output_ids)):\n",
    "            ids = output_ids[i]\n",
    "            output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "            assert len(output_ids[i]) == 1024\n",
    "    # print(\"6\")\n",
    "    return {\n",
    "        'input_ids': output_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "truncated 1 examples\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "660\n",
      "690\n",
      "truncated 3 examples\n",
      "720\n",
      "truncated 1 examples\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "truncated 1 examples\n",
      "870\n",
      "900\n",
      "truncated 1 examples\n",
      "930\n",
      "960\n",
      "truncated 1 examples\n",
      "990\n",
      "1020\n",
      "1050\n",
      "1080\n",
      "1110\n",
      "1140\n",
      "1170\n",
      "1200\n",
      "1230\n",
      "1260\n",
      "1290\n",
      "1320\n",
      "1350\n",
      "1380\n",
      "1410\n",
      "1440\n",
      "1470\n",
      "1500\n",
      "1530\n",
      "truncated 1 examples\n",
      "1560\n",
      "1590\n",
      "truncated 2 examples\n",
      "1620\n",
      "1650\n",
      "truncated 2 examples\n",
      "1680\n",
      "1710\n",
      "truncated 2 examples\n",
      "1740\n",
      "1770\n",
      "1800\n",
      "1830\n",
      "1860\n",
      "truncated 1 examples\n",
      "1890\n",
      "1920\n",
      "truncated 2 examples\n",
      "1950\n",
      "1980\n",
      "truncated 1 examples\n",
      "2010\n",
      "2040\n",
      "2070\n",
      "truncated 1 examples\n",
      "2100\n",
      "truncated 1 examples\n",
      "2130\n",
      "2160\n",
      "2190\n",
      "2220\n",
      "truncated 1 examples\n",
      "2250\n",
      "2280\n",
      "2310\n",
      "2340\n",
      "2370\n",
      "2400\n",
      "2430\n",
      "2460\n",
      "2490\n",
      "truncated 2 examples\n",
      "2520\n",
      "truncated 2 examples\n",
      "2550\n",
      "truncated 2 examples\n",
      "2580\n",
      "2610\n",
      "2640\n",
      "2670\n",
      "2700\n",
      "truncated 1 examples\n",
      "2730\n",
      "2760\n",
      "truncated 1 examples\n",
      "2790\n",
      "2820\n",
      "2850\n",
      "2880\n",
      "2910\n",
      "2940\n",
      "2970\n",
      "3000\n",
      "3030\n",
      "truncated 1 examples\n",
      "3060\n",
      "3090\n",
      "3120\n",
      "truncated 1 examples\n",
      "3150\n",
      "3180\n",
      "3210\n",
      "truncated 1 examples\n",
      "3240\n",
      "3270\n",
      "3300\n",
      "3330\n",
      "3360\n",
      "3390\n",
      "3420\n",
      "3450\n",
      "3480\n",
      "truncated 1 examples\n",
      "3510\n",
      "truncated 2 examples\n",
      "3540\n",
      "3570\n",
      "3600\n",
      "3630\n",
      "3660\n",
      "3690\n",
      "truncated 2 examples\n",
      "3720\n",
      "truncated 5 examples\n",
      "3750\n",
      "truncated 1 examples\n",
      "3780\n",
      "truncated 1 examples\n",
      "3810\n",
      "3840\n",
      "truncated 2 examples\n",
      "3870\n",
      "3900\n",
      "3930\n",
      "3960\n",
      "3990\n",
      "4020\n",
      "4050\n",
      "4080\n",
      "4110\n",
      "4140\n",
      "4170\n",
      "truncated 1 examples\n",
      "4200\n",
      "4230\n",
      "4260\n",
      "truncated 1 examples\n",
      "4290\n",
      "truncated 1 examples\n",
      "4320\n",
      "4350\n",
      "4380\n",
      "4410\n",
      "4440\n",
      "truncated 1 examples\n",
      "4470\n",
      "4500\n",
      "4530\n",
      "4560\n",
      "truncated 1 examples\n",
      "4590\n",
      "4620\n",
      "truncated 2 examples\n",
      "4650\n",
      "4680\n",
      "4710\n",
      "4740\n",
      "4770\n",
      "4800\n",
      "truncated 1 examples\n",
      "4830\n",
      "truncated 1 examples\n",
      "4860\n",
      "4890\n",
      "truncated 1 examples\n",
      "4920\n",
      "truncated 1 examples\n",
      "4950\n",
      "4980\n",
      "5010\n",
      "5040\n",
      "truncated 1 examples\n",
      "5070\n",
      "truncated 1 examples\n",
      "5100\n",
      "5130\n",
      "5160\n",
      "5190\n",
      "5220\n",
      "truncated 1 examples\n",
      "5250\n",
      "5280\n",
      "5310\n",
      "truncated 1 examples\n",
      "5340\n",
      "5370\n",
      "truncated 1 examples\n",
      "5400\n",
      "5430\n",
      "5460\n",
      "5490\n",
      "5520\n",
      "5550\n",
      "5580\n",
      "5610\n",
      "5640\n",
      "5670\n",
      "5700\n",
      "truncated 2 examples\n",
      "5730\n",
      "5760\n",
      "truncated 3 examples\n",
      "5790\n",
      "5820\n",
      "5850\n",
      "5880\n",
      "truncated 1 examples\n",
      "5910\n",
      "5940\n",
      "5970\n",
      "6000\n",
      "6030\n",
      "6060\n",
      "truncated 3 examples\n",
      "6090\n",
      "6120\n",
      "6150\n",
      "truncated 1 examples\n",
      "6180\n",
      "6210\n",
      "truncated 1 examples\n",
      "6240\n",
      "truncated 1 examples\n",
      "6270\n",
      "6300\n",
      "truncated 1 examples\n",
      "6330\n",
      "6360\n",
      "truncated 2 examples\n",
      "6390\n",
      "6420\n",
      "6450\n",
      "6480\n",
      "6510\n",
      "6540\n",
      "6570\n",
      "truncated 1 examples\n",
      "6600\n",
      "truncated 2 examples\n",
      "6630\n",
      "6660\n",
      "truncated 1 examples\n",
      "6690\n",
      "truncated 2 examples\n",
      "6720\n",
      "6750\n",
      "truncated 1 examples\n",
      "6780\n",
      "6810\n",
      "6840\n",
      "6870\n",
      "6900\n",
      "6930\n",
      "truncated 1 examples\n",
      "6960\n",
      "6990\n",
      "7020\n",
      "7050\n",
      "7080\n",
      "7110\n",
      "truncated 1 examples\n",
      "7140\n",
      "7170\n",
      "7200\n",
      "truncated 1 examples\n",
      "7230\n",
      "7260\n",
      "7290\n",
      "7320\n",
      "7350\n",
      "truncated 1 examples\n",
      "7380\n",
      "7410\n",
      "7440\n",
      "7470\n",
      "7500\n",
      "7530\n",
      "7560\n",
      "7590\n",
      "truncated 1 examples\n",
      "7620\n",
      "7650\n",
      "7680\n",
      "7710\n",
      "7740\n",
      "7770\n",
      "7800\n",
      "7830\n",
      "truncated 1 examples\n",
      "7860\n",
      "7890\n",
      "7920\n",
      "7950\n",
      "truncated 1 examples\n",
      "7980\n",
      "truncated 1 examples\n",
      "8010\n",
      "truncated 1 examples\n",
      "8040\n",
      "8070\n",
      "8100\n",
      "8130\n",
      "8160\n",
      "8190\n",
      "8220\n",
      "8250\n",
      "truncated 1 examples\n",
      "8280\n",
      "8310\n",
      "8340\n",
      "8370\n",
      "8400\n",
      "8430\n",
      "8460\n",
      "8490\n",
      "8520\n",
      "8550\n",
      "truncated 1 examples\n",
      "8580\n",
      "8610\n",
      "8640\n",
      "8670\n",
      "8700\n",
      "truncated 1 examples\n",
      "8730\n",
      "8760\n",
      "8790\n",
      "8820\n",
      "8850\n",
      "8880\n",
      "8910\n",
      "8940\n",
      "truncated 1 examples\n",
      "8970\n",
      "9000\n",
      "9030\n",
      "9060\n",
      "9090\n",
      "9120\n",
      "9150\n",
      "9180\n",
      "truncated 1 examples\n",
      "9210\n",
      "9240\n",
      "9270\n",
      "9300\n",
      "truncated 1 examples\n",
      "9330\n",
      "9360\n",
      "9390\n",
      "truncated 1 examples\n",
      "9420\n",
      "truncated 1 examples\n",
      "9450\n",
      "9480\n",
      "9510\n",
      "9540\n",
      "9570\n",
      "9600\n",
      "9630\n",
      "9660\n",
      "9690\n",
      "9720\n",
      "9750\n",
      "truncated 1 examples\n",
      "9780\n",
      "9810\n",
      "9840\n",
      "truncated 1 examples\n",
      "9870\n",
      "9900\n",
      "9930\n",
      "9960\n",
      "9990\n",
      "10020\n",
      "10050\n",
      "truncated 1 examples\n",
      "10080\n",
      "10110\n",
      "10140\n",
      "10170\n",
      "10200\n",
      "10230\n",
      "10260\n",
      "10290\n",
      "10320\n",
      "10350\n",
      "truncated 1 examples\n",
      "10380\n",
      "10410\n",
      "10440\n",
      "10470\n",
      "10500\n",
      "truncated 1 examples\n",
      "10530\n",
      "10560\n",
      "10590\n",
      "10620\n",
      "10650\n",
      "10680\n",
      "10710\n",
      "10740\n",
      "10770\n",
      "10800\n",
      "10830\n",
      "10860\n",
      "10890\n",
      "10920\n",
      "10950\n",
      "10980\n",
      "11010\n",
      "11040\n",
      "11070\n",
      "11100\n",
      "11130\n",
      "11160\n",
      "11190\n",
      "11220\n",
      "11250\n",
      "11280\n",
      "11310\n",
      "11340\n",
      "11370\n",
      "truncated 1 examples\n",
      "11400\n",
      "11430\n",
      "11460\n",
      "11490\n",
      "11520\n",
      "11550\n",
      "truncated 1 examples\n",
      "11580\n",
      "11610\n",
      "truncated 1 examples\n",
      "11640\n",
      "11670\n",
      "11700\n",
      "truncated 1 examples\n",
      "11730\n",
      "11760\n",
      "11790\n",
      "11820\n",
      "11850\n",
      "11880\n",
      "11910\n",
      "11940\n",
      "11970\n",
      "12000\n",
      "12030\n",
      "truncated 1 examples\n",
      "12060\n",
      "12090\n",
      "12120\n",
      "12150\n",
      "12180\n",
      "12210\n",
      "12240\n",
      "truncated 1 examples\n",
      "12270\n",
      "12300\n",
      "12330\n",
      "12360\n",
      "12390\n",
      "truncated 1 examples\n",
      "12420\n",
      "12450\n",
      "12480\n",
      "12510\n",
      "12540\n",
      "12570\n",
      "12600\n",
      "12630\n",
      "12660\n",
      "12690\n",
      "truncated 1 examples\n",
      "12720\n",
      "12750\n",
      "truncated 1 examples\n",
      "12780\n",
      "12810\n",
      "truncated 1 examples\n",
      "12840\n",
      "12870\n",
      "12900\n",
      "truncated 1 examples\n",
      "12930\n",
      "12960\n",
      "12990\n",
      "truncated 2 examples\n",
      "13020\n",
      "13050\n",
      "13080\n",
      "13110\n",
      "truncated 1 examples\n",
      "13140\n",
      "13170\n",
      "13200\n",
      "13230\n",
      "13260\n",
      "13290\n",
      "13320\n",
      "13350\n",
      "truncated 3 examples\n",
      "13380\n",
      "13410\n",
      "truncated 1 examples\n",
      "13440\n",
      "13470\n",
      "13500\n",
      "13530\n",
      "13560\n",
      "truncated 1 examples\n",
      "13590\n",
      "13620\n",
      "truncated 1 examples\n",
      "13650\n",
      "13680\n",
      "13710\n",
      "13740\n",
      "13770\n",
      "truncated 1 examples\n",
      "13800\n",
      "13830\n",
      "13860\n",
      "13890\n",
      "13920\n",
      "13950\n",
      "truncated 1 examples\n",
      "13980\n",
      "14010\n",
      "truncated 1 examples\n",
      "14040\n",
      "14070\n",
      "14100\n",
      "14130\n",
      "14160\n",
      "14190\n",
      "truncated 1 examples\n",
      "14220\n",
      "truncated 1 examples\n",
      "14250\n",
      "14280\n",
      "14310\n",
      "14340\n",
      "14370\n",
      "14400\n",
      "14430\n",
      "14460\n",
      "truncated 2 examples\n",
      "14490\n",
      "14520\n",
      "14550\n",
      "14580\n",
      "14610\n",
      "14640\n",
      "14670\n",
      "14700\n",
      "14730\n",
      "14760\n",
      "14790\n",
      "14820\n",
      "14850\n",
      "truncated 2 examples\n",
      "14880\n",
      "truncated 1 examples\n",
      "14910\n",
      "14940\n",
      "14970\n",
      "15000\n",
      "15030\n",
      "15060\n",
      "15090\n",
      "15120\n",
      "15150\n",
      "truncated 2 examples\n",
      "15180\n",
      "15210\n",
      "15240\n",
      "15270\n",
      "15300\n",
      "15330\n",
      "truncated 1 examples\n",
      "15360\n",
      "truncated 1 examples\n",
      "15390\n",
      "truncated 1 examples\n",
      "15420\n",
      "truncated 1 examples\n",
      "15450\n",
      "15480\n",
      "truncated 1 examples\n",
      "15510\n",
      "15540\n",
      "15570\n",
      "truncated 1 examples\n",
      "15600\n",
      "15630\n",
      "15660\n",
      "15690\n",
      "truncated 1 examples\n",
      "15720\n",
      "15750\n",
      "15780\n",
      "truncated 1 examples\n",
      "15810\n",
      "15840\n",
      "15870\n",
      "15900\n",
      "15930\n",
      "truncated 1 examples\n",
      "15960\n",
      "15990\n",
      "16020\n",
      "truncated 1 examples\n",
      "16050\n",
      "truncated 1 examples\n",
      "16080\n",
      "16110\n",
      "truncated 1 examples\n",
      "16140\n",
      "16170\n",
      "16200\n",
      "16230\n",
      "truncated 1 examples\n",
      "16260\n",
      "16290\n",
      "16320\n",
      "16350\n",
      "16380\n",
      "16410\n",
      "16440\n",
      "truncated 1 examples\n",
      "16470\n",
      "16500\n",
      "truncated 1 examples\n",
      "16530\n",
      "16560\n",
      "truncated 2 examples\n",
      "16590\n",
      "16620\n",
      "truncated 3 examples\n",
      "16650\n",
      "truncated 1 examples\n",
      "16680\n",
      "truncated 2 examples\n",
      "16710\n",
      "truncated 1 examples\n",
      "16740\n",
      "16770\n",
      "16800\n",
      "16830\n",
      "16860\n",
      "16890\n",
      "16920\n",
      "16950\n",
      "truncated 2 examples\n",
      "16980\n",
      "17010\n",
      "truncated 1 examples\n",
      "17040\n",
      "17070\n",
      "17100\n",
      "17130\n",
      "17160\n",
      "truncated 1 examples\n",
      "17190\n",
      "17220\n",
      "17250\n",
      "17280\n",
      "17310\n",
      "17340\n",
      "17370\n",
      "17400\n",
      "17430\n",
      "17460\n",
      "17490\n",
      "17520\n",
      "17550\n",
      "truncated 1 examples\n",
      "17580\n",
      "truncated 1 examples\n",
      "17610\n",
      "truncated 1 examples\n",
      "17640\n",
      "17670\n",
      "17700\n",
      "truncated 1 examples\n",
      "17730\n",
      "17760\n",
      "17790\n",
      "17820\n",
      "truncated 1 examples\n",
      "17850\n",
      "17880\n",
      "17910\n",
      "17940\n",
      "17970\n",
      "truncated 1 examples\n",
      "18000\n",
      "truncated 1 examples\n",
      "18030\n",
      "18060\n",
      "truncated 1 examples\n",
      "18090\n",
      "truncated 1 examples\n",
      "18120\n",
      "truncated 1 examples\n",
      "18150\n",
      "18180\n",
      "18210\n",
      "18240\n",
      "18270\n",
      "truncated 1 examples\n",
      "18300\n",
      "18330\n",
      "18360\n",
      "18390\n",
      "18420\n",
      "18450\n",
      "18480\n",
      "18510\n",
      "18540\n",
      "18570\n",
      "18600\n",
      "18630\n",
      "18660\n",
      "18690\n",
      "18720\n",
      "18750\n",
      "18780\n",
      "truncated 1 examples\n",
      "18810\n",
      "18840\n",
      "18870\n",
      "18900\n",
      "18930\n",
      "truncated 1 examples\n",
      "18960\n",
      "18990\n",
      "19020\n",
      "19050\n",
      "19080\n",
      "truncated 1 examples\n",
      "19110\n",
      "19140\n",
      "19170\n",
      "19200\n",
      "truncated 1 examples\n",
      "19230\n",
      "19260\n",
      "19290\n",
      "19320\n",
      "19350\n",
      "19380\n",
      "19410\n",
      "19440\n",
      "19470\n",
      "19500\n",
      "19530\n",
      "19560\n",
      "19590\n",
      "19620\n",
      "truncated 1 examples\n",
      "19650\n",
      "19680\n",
      "19710\n",
      "19740\n",
      "19770\n",
      "19800\n",
      "19830\n",
      "19860\n",
      "19890\n",
      "19920\n",
      "19950\n",
      "19980\n",
      "20010\n",
      "20040\n",
      "20070\n",
      "truncated 1 examples\n",
      "20100\n",
      "truncated 1 examples\n",
      "20130\n",
      "truncated 1 examples\n",
      "20160\n",
      "20190\n",
      "20220\n",
      "20250\n",
      "truncated 2 examples\n",
      "20280\n",
      "20310\n",
      "truncated 1 examples\n",
      "20340\n",
      "20370\n",
      "truncated 2 examples\n",
      "20400\n",
      "20430\n",
      "20460\n",
      "20490\n",
      "20520\n",
      "truncated 2 examples\n",
      "20550\n",
      "20580\n",
      "truncated 1 examples\n",
      "20610\n",
      "20640\n",
      "20670\n",
      "20700\n",
      "20730\n",
      "20760\n",
      "20790\n",
      "20820\n",
      "20850\n",
      "20880\n",
      "20910\n",
      "20940\n",
      "truncated 1 examples\n",
      "20970\n",
      "21000\n",
      "truncated 1 examples\n",
      "21030\n",
      "21060\n",
      "21090\n",
      "truncated 1 examples\n",
      "21120\n",
      "21150\n",
      "truncated 1 examples\n",
      "21180\n",
      "21210\n",
      "21240\n",
      "21270\n",
      "21300\n",
      "21330\n",
      "21360\n",
      "21390\n",
      "21420\n",
      "21450\n",
      "21480\n",
      "truncated 2 examples\n",
      "21510\n",
      "21540\n",
      "truncated 1 examples\n",
      "21570\n",
      "21600\n",
      "21630\n",
      "21660\n",
      "truncated 1 examples\n",
      "21690\n",
      "truncated 2 examples\n",
      "21720\n",
      "truncated 2 examples\n",
      "21750\n",
      "21780\n",
      "truncated 1 examples\n",
      "21810\n",
      "truncated 1 examples\n",
      "21840\n",
      "21870\n",
      "truncated 1 examples\n",
      "21900\n",
      "21930\n",
      "truncated 1 examples\n",
      "21960\n",
      "truncated 1 examples\n",
      "21990\n",
      "22020\n",
      "truncated 1 examples\n",
      "22050\n",
      "22080\n",
      "22110\n",
      "22140\n",
      "truncated 2 examples\n",
      "22170\n",
      "22200\n",
      "22230\n",
      "truncated 1 examples\n",
      "22260\n",
      "22290\n",
      "22320\n",
      "22350\n",
      "22380\n",
      "22410\n",
      "22440\n",
      "22470\n",
      "22500\n",
      "22530\n",
      "22560\n",
      "22590\n",
      "22620\n",
      "truncated 1 examples\n",
      "22650\n",
      "22680\n",
      "22710\n",
      "22740\n",
      "22770\n",
      "22800\n",
      "22830\n",
      "22860\n",
      "22890\n",
      "22920\n",
      "22950\n",
      "22980\n",
      "23010\n",
      "23040\n",
      "23070\n",
      "23100\n",
      "23130\n",
      "23160\n",
      "23190\n",
      "23220\n",
      "truncated 1 examples\n",
      "23250\n",
      "23280\n",
      "truncated 1 examples\n",
      "23310\n",
      "23340\n",
      "23370\n",
      "23400\n",
      "truncated 1 examples\n",
      "23430\n",
      "23460\n",
      "truncated 1 examples\n",
      "23490\n",
      "truncated 1 examples\n",
      "23520\n",
      "23550\n",
      "23580\n",
      "23610\n",
      "23640\n",
      "23670\n",
      "23700\n",
      "23730\n",
      "23760\n",
      "23790\n",
      "23820\n",
      "23850\n",
      "truncated 1 examples\n",
      "23880\n",
      "23910\n",
      "23940\n",
      "23970\n",
      "24000\n",
      "truncated 1 examples\n",
      "24030\n",
      "24060\n",
      "24090\n",
      "24120\n",
      "24150\n",
      "truncated 1 examples\n",
      "24180\n",
      "24210\n",
      "truncated 1 examples\n",
      "24240\n"
     ]
    }
   ],
   "source": [
    "# feed the dataset:dataset to the pro_processing_ner() function with tokenizer, at each time, we feed 30 examples to the function, and then save the output to a json file\n",
    "# each time the return of the function is a dict, we need to save the dict to a list, and then save the list to a json file\n",
    "\n",
    "import json\n",
    "\n",
    "output = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "for i in range(0, len(dataset), 30):\n",
    "    print(i)\n",
    "    result = pro_processing_ner(dataset[i:i+30], tokenizer)\n",
    "    output[\"input_ids\"].extend(result[\"input_ids\"])\n",
    "    output[\"attention_mask\"].extend(result[\"attention_mask\"])\n",
    "    \n",
    "\n",
    "with open('DocRED/data/train_ner.json', 'w') as f:\n",
    "    json.dump(output, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DocRED/data/train_ner.json') as f:\n",
    "    ner_dataset = json.load(f)\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids': ner_dataset['input_ids'],\n",
    "        'attention_mask': ner_dataset['attention_mask'],\n",
    "    }\n",
    ")\n",
    "\n",
    "# del ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jonas geirnaert ( born july 28, 1982 ) studied animation at the kask in ghent. in may 2004 he won the short film jury prize at the cannes film festival with his animated short flatlife ( 11 min ). [learn1] [learn2] entity : jonas geirnaert, type : head of government ; entity : july 28, 1982, type : time ; entity : kask, type : organization ; entity : ghent, type : location ; entity : may 2004, type : time ; entity : short film jury prize, type : miscellaneous ; entity : cannes film festival, type : miscellaneous ; entity : flatlife, type : miscellaneous ; entity : 11 min, type : number. [learn3] [learn4] relation 0 : 0 ; relation 1 : 0 ; relation 2 : 0 ; relation 3 : 0 ; relation 4 : 0 ; relation 5 : 0 ; relation 6 : 0 ; relation 7 : 0 ; relation 8 : 0 ; relation 9 : 0 ; relation 10 : 0 ; relation 11 : 0 ; relation 12 : 0 ; relation 13 : 0 ; relation 14 : 0 ; relation 15 : 0 ; relation 16 : 0 ; relation 17 : 0 ; relation 18 : 1 ; relation 19 : 0 ; relation 20 : 0 ; relation 21 : 0 ; relation 22 : 0 ; relation 23 : 0 ; relation 24 : 0 ; relation 25 : 0 ; relation 26 : 0 ; relation 27 : 0 ; relation 28 : 0 ; relation 29 : 0 ; relation 30 : 0 ; relation 31 : 0 ; relation 32 : 0 ; relation 33 : 0 ; relation 34 : 0 ; relation 35 : 0 ; relation 36 : 0 ; relation 37 : 0 ; relation 38 : 0 ; relation 39 : 0 ; relation 40 : 0 ; relation 41 : 0 ; relation 42 : 0 ; relation 43 : 1 ; relation 44 : 0 ; relation 45 : 0 ; relation 46 : 0 ; relation 47 : 0 ; relation 48 : 0 ; relation 49 : 0 ; relation 50 : 0 ; relation 51 : 0 ; relation 52 : 0 ; relation 53 : 0 ; relation 54 : 0 ; relation 55 : 0 ; relation 56 : 0 ; relation 57 : 0 ; relation 58 : 0 ; relation 59 : 0 ; relation 60 : 0 ; relation 61 : 0 ; relation 62 : 0 ; relation 63 : 0 ; relation 64 : 0 ; relation 65 : 0 ; relation 66 : 0 ; relation 67 : 0 ; relation 68 : 0 ; relation 69 : 0 ; relation 70 : 0 ; relation 71 : 0 ; relation 72 : 0 ; relation 73 : 0 ; relation 74 : 0 ; relation 75 : 0 ; relation 76 : 0 ; relation 77 : 0 ; relation 78 : 0 ; relation 79 : 0 ; relation 80 : 0 ; relation 81 : 0 ; relation 82 : 0 ; relation 83 : 0 ; relation 84 : 0 ; relation 85 : 0 ; relation 86 : 0 ; relation 87 : 0 ; relation 88 : 0 ; relation 89 : 0 ; relation 90 : 0 ; relation 91 : 0 ; relation 92 : 0 ; relation 93 : 0 ; relation 94 : 0 ; relation 95 : 0. [learn5] [learn6] for relation 18, head : jonas geirnaert, tail : july 28, 1982. for relation 43, head : kask, tail : ghent. <|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset[66]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([  270, 12228,  7530,  ..., 50258, 50258, 50258]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])},\n",
       " {'input_ids': tensor([ 1169, 18091,   373,  ..., 50258, 50258, 50258]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.__getitems__([1,4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/intermediate-sv/wandb/run-20230915_031710-bxqc0fcd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2-intermediate/runs/bxqc0fcd' target=\"_blank\">GPT2-DocRED-w-ner-5epochs</a></strong> to <a href='https://wandb.ai/tian1995/GPT2-intermediate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2-intermediate' target=\"_blank\">https://wandb.ai/tian1995/GPT2-intermediate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2-intermediate/runs/bxqc0fcd' target=\"_blank\">https://wandb.ai/tian1995/GPT2-intermediate/runs/bxqc0fcd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2-intermediate/runs/bxqc0fcd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fae2d336f50>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2-intermediate\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"GPT2-medium-DocRED-w-ner-5epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4, \n",
    "        # fp16=True,\n",
    "        logging_steps=100, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='DocRED/GPT_medium_w_ner'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/intermediate-sv/wandb/run-20230921_022503-2bpstxy1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/huggingface/runs/2bpstxy1' target=\"_blank\">faithful-water-31</a></strong> to <a href='https://wandb.ai/tian1995/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/huggingface' target=\"_blank\">https://wandb.ai/tian1995/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/huggingface/runs/2bpstxy1' target=\"_blank\">https://wandb.ai/tian1995/huggingface/runs/2bpstxy1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f113c4eed54265b21721b7aff3e4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.76 GiB total capacity; 10.65 GiB already allocated; 28.94 MiB free; 10.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/tian/Projects/intermediate-sv/try.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tian/Projects/intermediate-sv/try.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1669\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1940\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1943\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1944\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1945\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1946\u001b[0m ):\n\u001b[1;32m   1947\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/trainer.py:2767\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2766\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2767\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2768\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:212\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\u001b[39;00m\n\u001b[1;32m    211\u001b[0m attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mtype(value\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 212\u001b[0m attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_dropout(attn_weights)\n\u001b[1;32m    214\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.76 GiB total capacity; 10.65 GiB already allocated; 28.94 MiB free; 10.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c73d7a563143c8b786337d3ee0f8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▄▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>30320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1111</td></tr><tr><td>train/total_flos</td><td>6.337899528192e+16</td></tr><tr><td>train/train_loss</td><td>0.2572</td></tr><tr><td>train/train_runtime</td><td>26005.4341</td></tr><tr><td>train/train_samples_per_second</td><td>4.664</td></tr><tr><td>train/train_steps_per_second</td><td>1.166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT2-DocRED-w-ner-5epochs</strong> at: <a href='https://wandb.ai/tian1995/GPT2-intermediate/runs/bxqc0fcd' target=\"_blank\">https://wandb.ai/tian1995/GPT2-intermediate/runs/bxqc0fcd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230915_031710-bxqc0fcd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('DocRED/GPT_w_ner/tokenizer/tokenizer_config.json',\n",
       " 'DocRED/GPT_w_ner/tokenizer/special_tokens_map.json',\n",
       " 'DocRED/GPT_w_ner/tokenizer/vocab.json',\n",
       " 'DocRED/GPT_w_ner/tokenizer/merges.txt',\n",
       " 'DocRED/GPT_w_ner/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "trainer.save_model(\"DocRED/GPT_medium_w_ner\")\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"DocRED/GPT_w_ner/tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"DocRED/GPT_w_ner/model\"\n",
    "# checkpoint = \"DocRED/GPT_without_ner/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"DocRED/GPT_w_ner/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|pad|>',\n",
       " '[learn1]',\n",
       " '[learn2]',\n",
       " '[learn3]',\n",
       " '[learn4]',\n",
       " '[learn5]',\n",
       " '[learn6]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output all of the special tokens in the tokenizer\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again.entity : , type : miscellaneous.relation 0 : 0 ; relation 1 : 0 ; relation 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "# inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. [learn1] [learn2] entity :\", return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. [learn1] [learn2] entity : \", return_tensors=\"pt\", padding='max_length', max_length=1000)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=20, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/dev.json') as f:\n",
    "    test_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {\n",
    "    'text':[],\n",
    "    'entity': [],\n",
    "    'relation': []\n",
    "}\n",
    "id_count = 0\n",
    "\n",
    "def pro_processing_data(train_set=train_set):\n",
    "    for i in range(len(train_set)):\n",
    "        \n",
    "\n",
    "        # text\n",
    "        sent_pairs = []\n",
    "        for sent_index, sent in enumerate(train_set[i]['sents']):\n",
    "            sents = \"\"\n",
    "\n",
    "            # flatten the sent list\n",
    "            a = \" \".join(sent)\n",
    "            sents += a.lower() + \" \"\n",
    "\n",
    "            # and the next_sent if it exists\n",
    "            try:\n",
    "                next_sent = train_set[i]['sents'][sent_index + 1]\n",
    "                b = \" \".join(next_sent)\n",
    "                sents += b.lower() + \" \"\n",
    "            except:\n",
    "                pass\n",
    "            # post process the sents for some spaces\n",
    "            sents = sents.strip()\n",
    "            sents = sents.replace(\"  \", \" \")\n",
    "\n",
    "            relation_dict['text'].append(sents)\n",
    "                \n",
    "        del sents\n",
    "\n",
    "\n",
    "        # entity\n",
    "        entity = []\n",
    "        for index in range(len(train_set[i]['sents'])):\n",
    "            # focus on the current sent and the next sent if it exists\n",
    "            if index + 1 < len(train_set[i]['sents']):\n",
    "                next_index = index + 1\n",
    "            else:\n",
    "                next_index = index\n",
    "            # group the entities for every 2 sents, no repeated entities in one group\n",
    "            c_sent_entity_lists = []\n",
    "            next_sent_entity_lists = []\n",
    "            entity_for_each_2_sents = []\n",
    "\n",
    "            for entity_spans in train_set[i]['vertexSet']:\n",
    "                for item in entity_spans:\n",
    "                    entity_item = []\n",
    "                    # if neither in the current sent nor in the next sent, continue\n",
    "                    if item['sent_id'] != index and item['sent_id'] != next_index:\n",
    "                        continue\n",
    "                    # also store the first pos of the entity in the entity_item\n",
    "                    # it will look like this: [[entity_name, sent_index, pos1, ner_type]]\n",
    "                    entity_item = [item['name'].lower().strip(), item['sent_id'], item['pos'][0], ner_info[item['type']]]\n",
    "\n",
    "                    if entity_item[1] == index:\n",
    "                        c_sent_entity_lists.append(entity_item)\n",
    "                    else:\n",
    "                        next_sent_entity_lists.append(entity_item)\n",
    "\n",
    "            # sort the c_sent_entity_lists and next_sent_entity_lists by the pos in ascending order\n",
    "            c_sent_entity_lists.sort(key=lambda x: x[2])\n",
    "            if index != next_index:\n",
    "                next_sent_entity_lists.sort(key=lambda x: x[2])\n",
    "            \n",
    "            entity_list = []\n",
    "            for item in c_sent_entity_lists:\n",
    "                if item[0] not in entity_list:\n",
    "                    entity_list.append(item[0])\n",
    "                    entity_for_each_2_sents.append([item[0], item[3]])\n",
    "                \n",
    "            if index != next_index:\n",
    "                for item in next_sent_entity_lists:\n",
    "                    if item[0] not in entity_list:\n",
    "                        entity_list.append(item[0])\n",
    "                        entity_for_each_2_sents.append([item[0], item[3]])\n",
    "\n",
    "            relation_dict['entity'].append(entity_for_each_2_sents)\n",
    "\n",
    "        del entity_item\n",
    "        del c_sent_entity_lists\n",
    "        del next_sent_entity_lists\n",
    "        del entity_for_each_2_sents\n",
    "            \n",
    "\n",
    "        # relation pairs\n",
    "        relation_pairs = []\n",
    "\n",
    "        for index in range(len(train_set[i]['sents'])):\n",
    "            relation_pairs_for_each_2_sents = {}\n",
    "            # focus on the current sent and the next sent if it exists\n",
    "            if index + 1 < len(train_set[i]['vertexSet']):\n",
    "                next_index = index + 1\n",
    "            else:\n",
    "                next_index = index\n",
    "\n",
    "            # heads, tails: ['entity_name', start_pos]\n",
    "            for relation_item in train_set[i]['labels']:\n",
    "                heads = []\n",
    "                tails = []\n",
    "                \n",
    "                # head\n",
    "                head_exist = False\n",
    "                for head_span in train_set[i]['vertexSet'][relation_item['h']]:\n",
    "                    if head_span['sent_id'] == index or head_span['sent_id'] == next_index:\n",
    "                        heads.append([head_span['name'].lower().strip(), head_span['pos'][0]])\n",
    "                        head_exist = True\n",
    "                if not head_exist:\n",
    "                    continue\n",
    "        \n",
    "                # tail\n",
    "                tail_exist = False\n",
    "                for tail_span in train_set[i]['vertexSet'][relation_item['t']]:\n",
    "                    if tail_span['sent_id'] == index or tail_span['sent_id'] == next_index:\n",
    "                        tails.append([tail_span['name'].lower().strip(), tail_span['pos'][0]])\n",
    "                        tail_exist = True\n",
    "                if not tail_exist:\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                if relation_info[relation_item['r']] not in relation_pairs_for_each_2_sents.keys():\n",
    "                    relation_pairs_for_each_2_sents[relation_info[relation_item['r']]] = []\n",
    "                for head in heads:\n",
    "                    for tail in tails:\n",
    "                        relation_pairs_for_each_2_sents[relation_info[relation_item['r']]].append([head[0], tail[0]])\n",
    "\n",
    "            relation_dict['relation'].append(relation_pairs_for_each_2_sents)\n",
    "\n",
    "\n",
    "        \n",
    "    return relation_dict\n",
    "\n",
    "\n",
    "test_relation_dict = pro_processing_data(test_set)\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/bi-sent-pre-process_test.json', 'w') as f:\n",
    "    json.dump(test_relation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DocRED/data/ner_info.json') as f:\n",
    "    ner_info = json.load(f)\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    relation_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner(example, tokenizer, padding=True):\n",
    "    texts = example['text']\n",
    "\n",
    "    for index in range(len(texts)):\n",
    "        # entity extraction and NER\n",
    "        texts[index] = texts[index].lower().strip() + \" [learn1] [learn2] \"\n",
    "        for entity in example['entity'][index]:\n",
    "            texts[index] = texts[index] + \" entity : \" + entity[0] + \" , type : \" + entity[1] + \" ;\"\n",
    "        texts[index] = texts[index][:-1] + \".\"\n",
    "        # print(\"1\")\n",
    "        # add relation classificaiton\n",
    "        texts[index] = texts[index].lower().strip() + \" [learn3] [learn4]\"\n",
    "        for relation_type, relation_pair in example['relation'][index].items():\n",
    "            if relation_pair:\n",
    "                texts[index] = texts[index] + \" relation \" + str(relation_info_dict[relation_type]) + \" : 1 ;\"\n",
    "            else:\n",
    "                texts[index] = texts[index] + \" relation \" + str(relation_info_dict[relation_type]) + \" : 0 ;\"\n",
    "\n",
    "        texts[index] = texts[index][:-1] + \".\"\n",
    "\n",
    "        # add relation extraction\n",
    "        texts[index] = texts[index].lower().strip() + \" [learn5] [learn6]\"\n",
    "        # print(\"2\")\n",
    "        for relation_type, relation_pair in example['relation'][index].items():\n",
    "            if relation_pair:\n",
    "                # print(\"text: \", texts[index])\n",
    "                # print(\"relation_pair: \", relation_pair)\n",
    "                texts[index] = texts[index] + \" for relation \" + str(relation_info_dict[relation_type]) + \" ,\"\n",
    "                for pair in relation_pair:\n",
    "                    texts[index] = texts[index] + \" head : \" + pair[0] + \" , tail : \" + pair[1] + \";\"\n",
    "                texts[index] = texts[index][:-1] + \".\"\n",
    "                \n",
    "        if texts[index][-2:] != \"6]\":\n",
    "            texts[index] = texts[index][:-1] + \". \" + tokenizer.eos_token\n",
    "        else:\n",
    "            texts[index] = texts[index] + \". \" + tokenizer.eos_token\n",
    "\n",
    "        # print(\"text: \", texts[index])\n",
    "    # print(\"3\")\n",
    "    # print(texts[0])\n",
    "    output_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "    # print(\"4\")\n",
    "    count = 0\n",
    "    for i in range(len(output_ids)):\n",
    "        ids = output_ids[i]\n",
    "        if len(ids) > 1024:\n",
    "            output_ids[i] = output_ids[i][:1023] + [tokenizer.eos_token_id]\n",
    "            count += 1\n",
    "        assert len(output_ids[i]) <= 1024\n",
    "        attention_mask.append([1] * len(output_ids[i]) + [0] * (1024 - len(output_ids[i])))\n",
    "        assert len(attention_mask[i]) == 1024\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    # print(\"5\")\n",
    "    if padding:\n",
    "        for i in range(len(output_ids)):\n",
    "            ids = output_ids[i]\n",
    "            output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "            assert len(output_ids[i]) == 1024\n",
    "    # print(\"6\")\n",
    "    return {\n",
    "        'input_ids': output_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1166 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "truncated 1 examples\n",
      "120\n",
      "truncated 1 examples\n",
      "150\n",
      "180\n",
      "210\n",
      "truncated 2 examples\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "truncated 1 examples\n",
      "390\n",
      "truncated 1 examples\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "truncated 2 examples\n",
      "660\n",
      "690\n",
      "720\n",
      "truncated 1 examples\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "870\n",
      "900\n",
      "truncated 3 examples\n",
      "930\n",
      "960\n",
      "990\n",
      "truncated 1 examples\n",
      "1020\n",
      "1050\n",
      "truncated 2 examples\n",
      "1080\n",
      "1110\n",
      "1140\n",
      "truncated 1 examples\n",
      "1170\n",
      "truncated 1 examples\n",
      "1200\n",
      "1230\n",
      "1260\n",
      "1290\n",
      "1320\n",
      "1350\n",
      "truncated 1 examples\n",
      "1380\n",
      "1410\n",
      "truncated 1 examples\n",
      "1440\n",
      "1470\n",
      "truncated 1 examples\n",
      "1500\n",
      "1530\n",
      "1560\n",
      "1590\n",
      "1620\n",
      "1650\n",
      "1680\n",
      "1710\n",
      "1740\n",
      "1770\n",
      "truncated 1 examples\n",
      "1800\n",
      "1830\n",
      "1860\n",
      "truncated 4 examples\n",
      "1890\n",
      "1920\n",
      "truncated 2 examples\n",
      "1950\n",
      "1980\n",
      "2010\n",
      "2040\n",
      "truncated 1 examples\n",
      "2070\n",
      "2100\n",
      "2130\n",
      "truncated 2 examples\n",
      "2160\n",
      "2190\n",
      "2220\n",
      "2250\n",
      "2280\n",
      "2310\n",
      "2340\n",
      "2370\n",
      "2400\n",
      "2430\n",
      "truncated 1 examples\n",
      "2460\n",
      "2490\n",
      "2520\n",
      "2550\n",
      "2580\n",
      "2610\n",
      "2640\n",
      "2670\n",
      "truncated 1 examples\n",
      "2700\n",
      "2730\n",
      "2760\n",
      "truncated 2 examples\n",
      "2790\n",
      "2820\n",
      "2850\n",
      "2880\n",
      "2910\n",
      "2940\n",
      "2970\n",
      "3000\n",
      "3030\n",
      "3060\n",
      "3090\n",
      "3120\n",
      "3150\n",
      "3180\n",
      "3210\n",
      "3240\n",
      "3270\n",
      "3300\n",
      "truncated 1 examples\n",
      "3330\n",
      "3360\n",
      "3390\n",
      "3420\n",
      "truncated 1 examples\n",
      "3450\n",
      "3480\n",
      "3510\n",
      "3540\n",
      "3570\n",
      "3600\n",
      "3630\n",
      "truncated 1 examples\n",
      "3660\n",
      "3690\n",
      "3720\n",
      "3750\n",
      "3780\n",
      "3810\n",
      "3840\n",
      "3870\n",
      "3900\n",
      "3930\n",
      "3960\n",
      "3990\n",
      "4020\n",
      "truncated 3 examples\n",
      "4050\n",
      "4080\n",
      "4110\n",
      "4140\n",
      "4170\n",
      "4200\n",
      "4230\n",
      "4260\n",
      "4290\n",
      "4320\n",
      "truncated 1 examples\n",
      "4350\n",
      "4380\n",
      "4410\n",
      "4440\n",
      "4470\n",
      "4500\n",
      "4530\n",
      "4560\n",
      "4590\n",
      "truncated 1 examples\n",
      "4620\n",
      "4650\n",
      "4680\n",
      "4710\n",
      "4740\n",
      "4770\n",
      "4800\n",
      "4830\n",
      "4860\n",
      "4890\n",
      "truncated 1 examples\n",
      "4920\n",
      "truncated 1 examples\n",
      "4950\n",
      "4980\n",
      "5010\n",
      "5040\n",
      "truncated 2 examples\n",
      "5070\n",
      "5100\n",
      "truncated 2 examples\n",
      "5130\n",
      "5160\n",
      "5190\n",
      "5220\n",
      "5250\n",
      "5280\n",
      "5310\n",
      "5340\n",
      "5370\n",
      "5400\n",
      "5430\n",
      "truncated 1 examples\n",
      "5460\n",
      "5490\n",
      "5520\n",
      "5550\n",
      "5580\n",
      "5610\n",
      "truncated 1 examples\n",
      "5640\n",
      "5670\n",
      "5700\n",
      "5730\n",
      "truncated 1 examples\n",
      "5760\n",
      "5790\n",
      "5820\n",
      "5850\n",
      "truncated 3 examples\n",
      "5880\n",
      "5910\n",
      "5940\n",
      "5970\n",
      "6000\n",
      "6030\n",
      "6060\n",
      "6090\n",
      "truncated 1 examples\n",
      "6120\n",
      "6150\n",
      "6180\n",
      "6210\n",
      "6240\n",
      "6270\n",
      "6300\n",
      "6330\n",
      "6360\n",
      "truncated 1 examples\n",
      "6390\n",
      "6420\n",
      "6450\n",
      "6480\n",
      "6510\n",
      "6540\n",
      "6570\n",
      "6600\n",
      "6630\n",
      "6660\n",
      "6690\n",
      "6720\n",
      "6750\n",
      "6780\n",
      "6810\n",
      "6840\n",
      "6870\n",
      "6900\n",
      "6930\n",
      "6960\n",
      "6990\n",
      "7020\n",
      "7050\n",
      "7080\n",
      "7110\n",
      "7140\n",
      "truncated 1 examples\n",
      "7170\n",
      "truncated 1 examples\n",
      "7200\n",
      "7230\n",
      "7260\n",
      "7290\n",
      "7320\n",
      "7350\n",
      "7380\n",
      "7410\n",
      "truncated 1 examples\n",
      "7440\n",
      "7470\n",
      "7500\n",
      "truncated 1 examples\n",
      "7530\n",
      "7560\n",
      "truncated 1 examples\n",
      "7590\n",
      "7620\n",
      "truncated 1 examples\n",
      "7650\n",
      "7680\n",
      "7710\n",
      "7740\n",
      "7770\n",
      "truncated 1 examples\n",
      "7800\n",
      "7830\n",
      "truncated 1 examples\n",
      "7860\n",
      "7890\n",
      "7920\n",
      "7950\n",
      "7980\n",
      "truncated 1 examples\n",
      "8010\n",
      "8040\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ner = 1\n",
    "\n",
    "test_relation_dict = {}\n",
    "if ner:\n",
    "    with open('DocRED/data/bi-sent-pre-process_test.json') as f:\n",
    "        test_relation_dict = json.load(f)\n",
    "\n",
    "    test_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': test_relation_dict['text'],\n",
    "            'entity': test_relation_dict['entity'],\n",
    "            'relation': test_relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "output = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "for i in range(0, len(test_dataset), 30):\n",
    "    print(i)\n",
    "    result = pro_processing_ner(test_dataset[i:i+30], tokenizer)\n",
    "    output[\"input_ids\"].extend(result[\"input_ids\"])\n",
    "    output[\"attention_mask\"].extend(result[\"attention_mask\"])\n",
    "    \n",
    "\n",
    "with open('DocRED/data/test_ner.json', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly select test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "with open('DocRED/data/test_ner.json') as f:\n",
    "    ner_dataset = json.load(f)\n",
    "\n",
    "tokenized_test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids': ner_dataset['input_ids'],\n",
    "        'attention_mask': ner_dataset['attention_mask'],\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  2224\n",
      "kungliga hovkapellet (, the royal court orchestra ) is a swedish orchestra, originally part of the royal court in sweden's capital stockholm. its existence was first recorded in 1526. [learn1] [learn2] entity : kungliga hovkapellet, type : organization ; entity : the royal court orchestra, type : organization ; entity : royal court orchestra, type : organization ; entity : swedish, type : location ; entity : royal court, type : organization ; entity : sweden, type : location ; entity : stockholm, type : location ; entity : 1526, type : time. [learn3] [learn4] relation 0 : 0 ; relation 1 : 0 ; relation 2 : 0 ; relation 3 : 0 ; relation 4 : 1 ; relation 5 : 1 ; relation 6 : 0 ; relation 7 : 0 ; relation 8 : 0 ; relation 9 : 0 ; relation 10 : 0 ; relation 11 : 0 ; relation 12 : 0 ; relation 13 : 0 ; relation 14 : 1 ; relation 15 : 0 ; relation 16 : 0 ; relation 17 : 0 ; relation 18 : 0 ; relation 19 : 0 ; relation 20 : 0 ; relation 21 : 0 ; relation 22 : 0 ; relation 23 : 0 ; relation 24 : 0 ; relation 25 : 0 ; relation 26 : 0 ; relation 27 : 0 ; relation 28 : 0 ; relation 29 : 0 ; relation 30 : 0 ; relation 31 : 0 ; relation 32 : 0 ; relation 33 : 0 ; relation 34 : 0 ; relation 35 : 0 ; relation 36 : 0 ; relation 37 : 0 ; relation 38 : 0 ; relation 39 : 0 ; relation 40 : 0 ; relation 41 : 0 ; relation 42 : 0 ; relation 43 : 0 ; relation 44 : 0 ; relation 45 : 0 ; relation 46 : 0 ; relation 47 : 0 ; relation 48 : 0 ; relation 49 : 0 ; relation 50 : 0 ; relation 51 : 0 ; relation 52 : 0 ; relation 53 : 0 ; relation 54 : 0 ; relation 55 : 0 ; relation 56 : 0 ; relation 57 : 0 ; relation 58 : 0 ; relation 59 : 0 ; relation 60 : 0 ; relation 61 : 0 ; relation 62 : 0 ; relation 63 : 0 ; relation 64 : 0 ; relation 65 : 0 ; relation 66 : 0 ; relation 67 : 0 ; relation 68 : 0 ; relation 69 : 0 ; relation 70 : 0 ; relation 71 : 0 ; relation 72 : 0 ; relation 73 : 0 ; relation 74 : 0 ; relation 75 : 0 ; relation 76 : 0 ; relation 77 : 0 ; relation 78 : 0 ; relation 79 : 0 ; relation 80 : 0 ; relation 81 : 0 ; relation 82 : 0 ; relation 83 : 0 ; relation 84 : 0 ; relation 85 : 0 ; relation 86 : 0 ; relation 87 : 0 ; relation 88 : 0 ; relation 89 : 0 ; relation 90 : 0 ; relation 91 : 0 ; relation 92 : 0 ; relation 93 : 0 ; relation 94 : 0 ; relation 95 : 0. [learn5] [learn6] for relation 4, head : sweden, tail : stockholm. for relation 5, head : stockholm, tail : sweden. for relation 14, head : stockholm, tail : sweden; head : royal court, tail : sweden. <|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "kungliga hovkapellet (, the royal court orchestra ) is a swedish orchestra, originally part of the royal court in sweden's capital stockholm. its existence was first recorded in 1526. [learn1] [learn2] entity : kungliga hovkapellet, type : organization ; entity : the royal court orchestra, type : organization ; entity : royal court orchestra, type : organization ; entity : swedish, type : location ; entity : royal court, type : organization ; entity : sweden, type : location ; entity : stockholm, type : location ; entity : 1526, type : time. [learn3] [learn4] relation 0 : 0 ; relation 1 : 0 ; relation 2 : 0 ; relation 3 : 0 ; relation 4 : 1 ; relation 5 : 1 ; relation 6 : 0 ; relation 7 : 0 ; relation 8 : 0 ; relation 9 : 0 ; relation 10 : 0 ; relation 11 : 0 ; relation 12 : 0 ; relation 13 : 0 ; relation 14 : 1 ; relation 15 : 0 ; relation 16 : 0 ; relation 17 : 0 ; relation 18 : 0 ; relation 19 : 0 ; relation 20 : 0 ; relation 21 : 0 ; relation 22 : 0 ; relation 23 : 0 ; relation 24 : 0 ; relation 25 : 0 ; relation 26 : 0 ; relation 27 : 0 ; relation 28 : 0 ; relation 29 : 0 ; relation 30 : 0 ; relation 31 : 0 ; relation 32 : 0 ; relation 33 : 0 ; relation 34 : 0 ; relation 35 : 0 ; relation 36 : 0 ; relation 37 : 0 ; relation 38 : 0 ; relation 39 : 0 ; relation 40 : 0 ; relation 41 : 0 ; relation 42 : 0 ; relation 43 : 0 ; relation 44 : 0 ; relation 45 : 0 ; relation 46 : 0 ; relation 47 : 0 ; relation 48 : 0 ; relation 49 : 0 ; relation 50 : 0 ; relation 51 : 0 ; relation 52 : 0 ; relation 53 : 0 ; relation 54 : 0 ; relation 55 : 0 ; relation 56 : 0 ; relation 57 : 0 ; relation 58 : 0 ; relation 59 : 0 ; relation 60 : 0 ; relation 61 : 0 ; relation 62 : 0 ; relation 63 : 0 ; relation 64 : 0 ; relation 65 : 0 ; relation 66 : 0 ; relation 67 : 0 ; relation 68 : 0 ; relation 69 : 0 ; relation 70 : 0 ; relation 71 : 0 ; relation 72 : 0 ; relation 73 : 0 ; relation 74 : 0 ; relation 75 : 0 ; relation 76 : 0 ; relation 77 : 0 ; relation 78 : 0 ; relation 79 : 0 ; relation 80 : 0 ; relation 81 : 0 ; relation 82 : 0 ; relation 83 : 0 ; relation 84 : 0 ; relation 85 : 0 ; relation 86 : 0 ; relation 87 : 0 ; relation 88 : 0 ; relation 89 : 0 ; relation 90 : 0 ; relation 91 : 0 ; relation 92 : 0 ; relation 93 : 0 ; relation 94 : 0 ; relation 95 : 0. [learn5] [learn6] for relation 4, head : sweden, tail : stockholm. for relation 5, head : stockholm, tail : sweden. for relation 14, head : stockholm, tail : sweden; head : royal court, tail : sweden. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# random sample 1 example from the test_dataset\n",
    "import random\n",
    "# have a random seed\n",
    "random.seed(80)\n",
    "\n",
    "\n",
    "index = random.randint(0, len(tokenized_test_dataset))\n",
    "print(\"index: \", index)\n",
    "print(tokenizer.decode(tokenized_test_dataset[index]['input_ids']))\n",
    "\n",
    "# output the length of tokenized_test_dataset[index]['input_ids'] except the padding tokens. the tokenized_test_dataset[index]['input_ids'] is tensor\n",
    "\n",
    "input_ids_list = tokenized_test_dataset[index]['input_ids'].tolist()\n",
    "valid_length = len(input_ids_list) - input_ids_list.count(tokenizer.pad_token_id)\n",
    "\n",
    "print(tokenizer.decode(tokenized_test_dataset[index]['input_ids'][:valid_length]))\n",
    "\n",
    "# generate a lower triangle matrix of 1s with the shape is (valid_length, valid_length), using torch\n",
    "\n",
    "low_triangle_matrix = torch.tril(torch.ones((valid_length, valid_length), dtype=torch.long))\n",
    "\n",
    "# find the index of the token id of \"[learn2]\" in the tokenized_test_dataset[index]['input_ids'] tensor\n",
    "\n",
    "learn2_index = tokenized_test_dataset[index]['input_ids'].tolist().index(tokenizer.convert_tokens_to_ids(\"[learn2]\"))\n",
    "\n",
    "# have a vector to store the token of okenized_test_dataset[index]['input_ids'][learn2_index + 1:valid_length]\n",
    "\n",
    "gold_truth = tokenized_test_dataset[index]['input_ids'][learn2_index + 1:valid_length]\n",
    "\n",
    "# multiply the low_triangle_matrix with the tokenized_test_dataset[index]['input_ids'][:valid_length]\n",
    "\n",
    "batch_input = low_triangle_matrix * tokenized_test_dataset[index]['input_ids'][:valid_length]\n",
    "\n",
    "# making all 0 in batch_input to tokenizer.pad_token_id\n",
    "\n",
    "batch_input[batch_input == 0] = tokenizer.pad_token_id\n",
    "\n",
    "# input from the [learn2] token\n",
    "\n",
    "actually_input = batch_input[learn2_index:-1]\n",
    "\n",
    "# have tensors actually_input_attention_mask, when the token is not padding token, the value is 1, otherwise, the value is 0\n",
    "\n",
    "actually_input_attention_mask = torch.ones(actually_input.shape, dtype=torch.long)\n",
    "actually_input_attention_mask[actually_input == tokenizer.pad_token_id] = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831213644c224020aa9ef71caa51e748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "\n",
    "batch_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # feed the actually_input to the model by 10 examples each time\n",
    "    for i in tqdm(range(0, len(actually_input), 10)):\n",
    "        print(i)\n",
    "        output = model(input_ids=actually_input[i:i+10].to(\"cuda\"), attention_mask=actually_input_attention_mask[i:i+10].to(\"cuda\"))\n",
    "        current_output = np.array(output['logits'].cpu())\n",
    "        max_index = np.argmax(current_output[:, -1, :], axis=1)\n",
    "        batch_output.extend(max_index)\n",
    "        # break\n",
    "\n",
    "    # print(tokenizer.batch_decode(max_index, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_output) == len(gold_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "\n",
    "accuracy = sum(np.array(batch_output) == np.array(gold_truth.tolist())) / len(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024154589371980676"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"kungliga hovkapellet (, the royal court orchestra ) is a swedish orchestra, originally part of the royal court in sweden's capital stockholm. its existence was first recorded in 1526. [learn1] [learn2] entity : kungliga hovkapellet, type : organization ; entity : the royal court orchestra, type : organization ; entity : royal court orchestra, type : organization ; entity : swedish, type : location ; entity : royal court, type : organization ; entity : sweden, type : location ; entity : stockholm, type : location ; entity : 1526, type : time. [learn3] [learn4] relation 0 : 0 ; relation 1 : 0 ; relation 2 : 0 ; relation 3 : 0 ; relation 4 : 1 ; relation 5 : 1 ; relation 6 : 0 ; relation 7 : 0 ; relation 8 : 0 ; relation 9 : 0 ; relation 10 : 0 ; relation 11 : 0 ; relation 12 : 0 ; relation 13 : 0 ; relation 14 : 1 ; relation 15 : 0 ; relation 16 : 0 ; relation 17 : 0 ; relation 18 : 0 ; relation 19 : 0 ; relation 20 : 0 ; relation 21 : 0 ; relation 22 : 0 ; relation 23 : 0 ; relation 24 : 0 ; relation 25 : 0 ; relation 26 : 0 ; relation 27 : 0 ; relation 28 : 0 ; relation 29 : 0 ; relation 30 : 0 ; relation 31 : 0 ; relation 32 : 0 ; relation 33 : 0 ; relation 34 : 0 ; relation 35 : 0 ; relation 36 : 0 ; relation 37 : 0 ; relation 38 : 0 ; relation 39 : 0 ; relation 40 : 0 ; relation 41 : 0 ; relation 42 : 0 ; relation 43 : 0 ; relation 44 : 0 ; relation 45 : 0 ; relation 46 : 0 ; relation 47 : 0 ; relation 48 : 0 ; relation 49 : 0 ; relation 50 : 0 ; relation 51 : 0 ; relation 52 : 0 ; relation 53 : 0 ; relation 54 : 0 ; relation 55 : 0 ; relation 56 : 0 ; relation 57 : 0 ; relation 58 : 0 ; relation 59 : 0 ; relation 60 : 0 ; relation 61 : 0 ; relation 62 : 0 ; relation 63 : 0 ; relation 64 : 0 ; relation 65 : 0 ; relation 66 : 0 ; relation 67 : 0 ; relation 68 : 0 ; relation 69 : 0 ; relation 70 : 0 ; relation 71 : 0 ; relation 72 : 0 ; relation 73 : 0 ; relation 74 : 0 ; relation 75 : 0 ; relation 76 : 0 ; relation 77 : 0 ; relation 78 : 0 ; relation 79 : 0 ; relation 80 : 0 ; relation 81 : 0 ; relation 82 : 0 ; relation 83 : 0 ; relation 84 : 0 ; relation 85 : 0 ; relation 86 : 0 ; relation 87 : 0 ; relation 88 : 0 ; relation 89 : 0 ; relation 90 : 0 ; relation 91 : 0 ; relation 92 : 0 ; relation 93 : 0 ; relation 94 : 0 ; relation 95 : 0. [learn5] [learn6] for relation 4, head : sweden, tail : stockholm. for relation 5, head : stockholm, tail : sweden. for relation 14, head : stockholm, tail : sweden; head : royal court, tail : sweden. <|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 :  ,\n",
      "51 :  ,\n",
      "60 :  ,\n",
      "69 :  ,\n",
      "84 :  [learn4]\n",
      "89 :   ;\n",
      "566 :  [learn6]\n",
      "567 :  for\n",
      "584 :  ,\n",
      "589 :  ,\n",
      "598 :  ,\n",
      "603 :  ,\n",
      "611 :   royal\n",
      "612 :   court\n",
      "613 :  ,\n"
     ]
    }
   ],
   "source": [
    "# output the index of the correct prediction\n",
    "\n",
    "for index, item in enumerate(zip(batch_output, gold_truth.tolist())):\n",
    "    if item[0] == item[1]:\n",
    "        print(index, \": \", tokenizer.decode(item[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_truth\n",
    "with open(\"DocRED/GPT_w_ner[]/result/epoch_5_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length: 12275, 12275\n",
      "instance:\n",
      "('1', '2', 'P17')\n",
      "('1', '2', 'P17')\n"
     ]
    }
   ],
   "source": [
    "print(f'the length: {len(result[\"output\"])}, {len(result[\"label\"])}')\n",
    "print(f'instance:\\n{result[\"output\"][0]}\\n{result[\"label\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and target, relation\n",
    "st_tp = 0\n",
    "st_fp = 0\n",
    "st_fn = 0\n",
    "st_tn = 0\n",
    "\n",
    "r_tp = 0\n",
    "r_fp = 0\n",
    "r_fn = 0\n",
    "r_tn = 0\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    pair = False\n",
    "    relation = False\n",
    "    if output[0] == label[0] and output[1] == label[1]:\n",
    "        st_tp += 1\n",
    "        pair = True\n",
    "    else:\n",
    "        st_fn += 1\n",
    "        st_fp += 1\n",
    "    \n",
    "    if output[2] == label[2]:\n",
    "        r_tp += 1\n",
    "        relation = True\n",
    "    else:\n",
    "        r_fn += 1\n",
    "        r_fp += 1\n",
    "\n",
    "    if pair and relation:\n",
    "        tuple_tp += 1\n",
    "    else:\n",
    "        tuple_fn += 1\n",
    "        tuple_fp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source and target precision: 0.8537678207739308, recall: 0.8537678207739308, f1: 0.8537678207739308\n",
      "relation precision: 0.7141344195519348, recall: 0.7141344195519348, f1: 0.7141344195519348\n",
      "tuple precision: 0.6980855397148676, recall: 0.6980855397148676, f1: 0.6980855397148676\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for source and target\n",
    "st_precision = st_tp / (st_tp + st_fp)\n",
    "st_recall = st_tp / (st_tp + st_fn)\n",
    "st_f1 = 2 * st_precision * st_recall / (st_precision + st_recall)\n",
    "print(f\"source and target precision: {st_precision}, recall: {st_recall}, f1: {st_f1}\")\n",
    "\n",
    "# for relation\n",
    "r_precision = r_tp / (r_tp + r_fp)\n",
    "r_recall = r_tp / (r_tp + r_fn)\n",
    "r_f1 = 2 * r_precision * r_recall / (r_precision + r_recall)\n",
    "print(f\"relation precision: {r_precision}, recall: {r_recall}, f1: {r_f1}\")\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
