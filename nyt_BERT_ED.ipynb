{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741629848f6b333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:48:26.564482155Z",
     "start_time": "2023-10-04T14:48:26.273204735Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be2b53a57ede7c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cac681bbdb8a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:49:44.545104785Z",
     "start_time": "2023-10-04T14:49:34.194846703Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=25, problem_type = \"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc810215d67827a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:49:53.094422653Z",
     "start_time": "2023-10-04T14:49:52.984532698Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "outputid2label = {0: 'B', 1: 'IN', 2: 'OUT'}\n",
    "outputlabel2id = {'B': 0, 'IN': 1, 'OUT': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3ff1a72e87e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:16:27.665766987Z",
     "start_time": "2023-10-04T15:16:26.582190222Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.config = self.bert.config\n",
    "\n",
    "    def forward(self, input_ids, \n",
    "                attention_mask=None, \n",
    "                token_type_ids=None,\n",
    "                labels=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,):\n",
    "        \n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,)\n",
    "        \n",
    "        pooled_output = outputs[0]\n",
    "\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                        labels.view(-1))\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "model = MultiLabelClassifier(num_labels=len(outputid2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e3292b5c0d9831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:50:55.048116469Z",
     "start_time": "2023-10-04T14:50:54.995133794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nyt/BERT_ED/tokenizer_config.json',\n",
       " 'nyt/BERT_ED/special_tokens_map.json',\n",
       " 'nyt/BERT_ED/vocab.txt',\n",
       " 'nyt/BERT_ED/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "\n",
    "tokenizer.save_pretrained('nyt/BERT_ED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63289504e436db",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e85b67a10794e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:54:10.966822849Z",
     "start_time": "2023-10-04T14:54:10.153170122Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "# with open('nyt/train.json') as f:\n",
    "#     train_set = json.load(f)\n",
    "\n",
    "with open('nyt/train.json') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('nyt/words2id.json') as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "\n",
    "with open('nyt/relations2id.json') as f:\n",
    "    rel2id_original = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6dec536708a947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:54:26.271713838Z",
     "start_time": "2023-10-04T14:54:26.221724644Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id2head_type = {}\n",
    "id2tail_type = {}\n",
    "id2rel = {}\n",
    "\n",
    "for k, v in rel2id_original.items():\n",
    "    if k == \"None\":\n",
    "        id2head_type[v] = \"None\"\n",
    "        id2tail_type[v] = \"None\"\n",
    "        id2rel[v] = \"None\"\n",
    "\n",
    "    else:\n",
    "        id2head_type[v] = k.split('/')[1]\n",
    "        id2tail_type[v] = k.split('/')[2]\n",
    "        id2rel[v] = k.split('/')[3]\n",
    "\n",
    "rel2id = {v: k for k, v in id2rel.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a59c8e760db03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "the structure of train_set is:\n",
    "\n",
    "[length]\n",
    "[words]\n",
    "[head, tail, relation, head, tail, relation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfad8c5e574a62",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "relation_dict = {\n",
    "    'text':[],\n",
    "    'entity': [],\n",
    "    'relation': []\n",
    "}\n",
    "id_count = 0\n",
    "\n",
    "for i in tqdm(range(len(train_set[0]))):\n",
    "    \n",
    "\n",
    "    # text\n",
    "    sent = \"\"\n",
    "    for sent_index, word_id in enumerate(train_set[1][i]):\n",
    "        word = id2word[word_id]\n",
    "        # flatten the sent list\n",
    "        sent += word + \" \"\n",
    "\n",
    "    # post process the sents for some spaces\n",
    "    sents = sent.strip()\n",
    "    sents = sent.replace(\"  \", \" \")\n",
    "\n",
    "    relation_dict['text'].append(sent)\n",
    "            \n",
    "    # del sents\n",
    "\n",
    "\n",
    "    # entity\n",
    "    entity = []\n",
    "    entity_for_order = []\n",
    "\n",
    "    # relations\n",
    "    relation_pairs = {rel:[] for rel in rel2id.keys()}\n",
    "\n",
    "    for j in range(0, len(train_set[2][i]), 3):\n",
    "        head_index = train_set[2][i][j]\n",
    "        head = train_set[1][i][head_index]\n",
    "\n",
    "        tail_index = train_set[2][i][j + 1]\n",
    "        tail = train_set[1][i][tail_index]\n",
    "\n",
    "        relation = id2rel[train_set[2][i][j + 2]]\n",
    "\n",
    "        if (head_index, id2head_type[train_set[2][i][j + 2]]) not in entity_for_order:\n",
    "            entity_for_order.append((head_index, id2head_type[train_set[2][i][j + 2]]))\n",
    "        if (tail_index, id2tail_type[train_set[2][i][j + 2]]) not in entity_for_order:\n",
    "            entity_for_order.append((tail_index, id2tail_type[train_set[2][i][j + 2]]))\n",
    "        \n",
    "        relation_pairs[relation].append((id2word[train_set[1][i][head_index]], id2word[train_set[1][i][tail_index]]))\n",
    "\n",
    "    # reorder the entity_for_order by the first element of the tuple\n",
    "    entity_for_order.sort(key=lambda x: x[0])\n",
    "    for tuple_item in entity_for_order:\n",
    "        entity.append((id2word[train_set[1][i][tuple_item[0]]], tuple_item[1]))\n",
    "    \n",
    "    for j in rel2id.keys():\n",
    "        if relation_pairs[j] == []:\n",
    "            relation_pairs[j] = None\n",
    "\n",
    "    # del entity\n",
    "    # del entity_for_order\n",
    "\n",
    "\n",
    "    relation_dict['entity'].append(entity)\n",
    "\n",
    "    relation_dict['relation'].append(relation_pairs)\n",
    "\n",
    "\n",
    "    \n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "# with open('nyt/data/sent-pre-process.json', 'w') as f:\n",
    "    # json.dump(relation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae10f118fa6349d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(relation_dict['text']) == len(relation_dict['entity']) == len(relation_dict['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f37196ce2325b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:57:04.523684506Z",
     "start_time": "2023-10-04T14:56:57.881941704Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ner = 1\n",
    "from datasets import Dataset\n",
    "\n",
    "relation_dict = {}\n",
    "if ner:\n",
    "    with open('nyt/data/sent-pre-process.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'entity': relation_dict['entity'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a529da56f43685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:58:07.932457232Z",
     "start_time": "2023-10-04T14:58:07.927134512Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B', 1: 'IN', 2: 'OUT'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputid2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfff76980450abcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:59:57.216882128Z",
     "start_time": "2023-10-04T14:59:57.156895482Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def is_subset(a, b):\n",
    "    if len(b) > len(a):\n",
    "        raise ValueError(\"entity is longer than text\")\n",
    "    else:\n",
    "        for i in range(len(a) - len(b) + 1):\n",
    "            if a[i:i + len(b)] == b:\n",
    "                return True, i\n",
    "        raise ValueError(\"entity is not in text\")\n",
    "\n",
    "def pro_processing_ner(example):\n",
    "    texts = example['text']\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "    for index in range(len(texts)):\n",
    "        # entity extraction and NER\n",
    "        text = texts[index].strip()\n",
    "        text_w_relation = f\"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "        tokenized_text = tokenizer.tokenize(text_w_relation)\n",
    "\n",
    "        tokenized_entity = [tokenizer.tokenize(entity) for entity, _ in example['entity'][index]]\n",
    "\n",
    "        label = [ [2] * len(tokenized_text) + [-100] * (512 - len(tokenized_text))]\n",
    "        label = label[0]\n",
    "        \n",
    "        for entity in tokenized_entity:\n",
    "            _, index = is_subset(tokenized_text, entity)\n",
    "            \n",
    "            label[index] = 0\n",
    "            if len(entity) > 1:\n",
    "                label[index + 1: index + len(entity)] = [1] * (len(entity) - 1)\n",
    "\n",
    "\n",
    "        input_texts.append(text_w_relation)\n",
    "        labels.append(label)\n",
    "    return {\n",
    "        'input_ids': input_texts, \n",
    "        'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ac42b45aeefd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:00:39.072161287Z",
     "start_time": "2023-10-04T15:00:07.766318910Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1874/1874 [00:31<00:00, 60.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# feed the dataset:dataset to the pro_processing_ner() function with tokenizer, at each time, we feed 30 examples to the function, and then save the output to a json file\n",
    "# each time the return of the function is a dict, we need to save the dict to a list, and then save the list to a json file\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output = {\"input_texts\": []}\n",
    "labels = {\"labels\": []}\n",
    "\n",
    "for i in tqdm(range(0, len(dataset), 30)):\n",
    "    result = pro_processing_ner(dataset[i:i+30])\n",
    "    output[\"input_texts\"].extend(result[\"input_ids\"])\n",
    "    labels[\"labels\"].extend(result[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79621c488d2bfa8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:01:30.910812719Z",
     "start_time": "2023-10-04T15:01:30.901416542Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] United States Representative Charles B. Rangel said yesterday that he would endorse C. Virginia Fields in New York City 's Democratic mayoral primary , giving her a jolt of momentum and delivering a setback to the efforts of Fernando Ferrer , the former Bronx borough president , to win black support . [SEP]\n",
      "[2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "num = 7\n",
    "\n",
    "print(output[\"input_texts\"][num])\n",
    "print(labels[\"labels\"][num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baaa140ceff458a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:01:44.269131537Z",
     "start_time": "2023-10-04T15:01:41.921968199Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make the output[\"input_texts\"] into a dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "input_text_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'input_texts': output['input_texts'],\n",
    "    }\n",
    ")\n",
    "\n",
    "labels_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'labels': labels['labels'],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ae168da272401b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:02:18.086526264Z",
     "start_time": "2023-10-04T15:01:47.400051454Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f53b22ba197458294f0ea1ebd2f3b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = input_text_dataset.map(lambda example: tokenizer(example['input_texts'], padding='max_length', add_special_tokens=False, truncation=True, max_length=512, pad_to_max_length=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbecce630fee4941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:02:58.995923600Z",
     "start_time": "2023-10-04T15:02:18.120497407Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# add a new column to the tokenized_dataset\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.add_column('labels', labels['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab96227387d0b40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:04:00.867382835Z",
     "start_time": "2023-10-04T15:04:00.752502155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_texts', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 56195\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ba7e712c4a655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:04:23.987945240Z",
     "start_time": "2023-10-04T15:04:21.224735936Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fca9012e92541029e731a00161a1fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/56195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove the column of input_texts in the tokenized_dataset\n",
    "tokenized_dataset.remove_columns('input_texts')\n",
    "# save the tokenized_dataset\n",
    "\n",
    "tokenized_dataset.save_to_disk('nyt/BERT_ED/train_data_ED')\n",
    "# with open('DocRED/data/train_ner_short_relation.json', 'w') as f:\n",
    "#     json.dump(tokenized_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b703cfd56ad35d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:37:08.721173289Z",
     "start_time": "2023-10-04T15:37:08.519301516Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_dataset = Dataset.load_from_disk('nyt/BERT_ED/train_data_ED')\n",
    "tokenized_dataset = tokenized_dataset.remove_columns('input_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76de5b41a927142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:37:09.499791642Z",
     "start_time": "2023-10-04T15:37:09.461050569Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1480739caf728354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:05:15.835980147Z",
     "start_time": "2023-10-04T15:05:15.584436774Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([  101,  1456,  2938, 23616,  9272,  9637,  2249,   150, 13329,  9741,\n",
       "            143,  9919, 21669, 12152,  2162, 22412, 10090,   117,  1340,  1512,\n",
       "            118,  1351,  1476,   119,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'labels': tensor([   2,    2,    0,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    0,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100])},\n",
       " {'input_ids': tensor([  101,  1109,  1244,  1311,  9088,  1106,  2470,   117,  3270,   144,\n",
       "           1813,  3293,   117,  1163,  1107,   170,  4195,  1115,  1119,  1125,\n",
       "           2002,  1103,  1237, 23184,  2193,  1107, 26118,  2001, 20792,  1106,\n",
       "           1231, 15622,  1179,  1113,  6356,   117,   170,  1989,  1170,  1119,\n",
       "           2802,  1122,  1804,  1272,  1104,   112,   112, 14207,  2861,  4289,\n",
       "            112,   112,  1373,  1103,  2350,  3070,   117,  1259,   170,  2560,\n",
       "           2321,  1107,  1103,  1331,  1107,  1134,  1594,  3384,  3850, 21813,\n",
       "           1215,   171, 10961,  9753,  2225,   119,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'labels': tensor([   2,    2,    2,    2,    2,    2,    0,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    0,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100])}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.__getitems__([1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f0ced99aba7998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:16:32.884260773Z",
     "start_time": "2023-10-04T15:16:30.279489522Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokenized_dataset['input_ids'][0:2], attention_mask=tokenized_dataset['attention_mask'][0:2], labels=tokenized_dataset['labels'][0:2])\n",
    "    # outputs = model(input_ids=tokenized_dataset['input_ids'][0:2], attention_mask=tokenized_dataset['attention_mask'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ba2a82a138a936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:16:56.227823281Z",
     "start_time": "2023-10-04T15:16:56.058447291Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2252)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ceb777a3ddde",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "895409e10c00d25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:32:41.585305637Z",
     "start_time": "2023-10-04T15:32:40.933937080Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "# with open('nyt/train.json') as f:\n",
    "#     train_set = json.load(f)\n",
    "\n",
    "with open('nyt/train.json') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('nyt/words2id.json') as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "\n",
    "with open('nyt/relations2id.json') as f:\n",
    "    rel2id_original = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "id2head_type = {}\n",
    "id2tail_type = {}\n",
    "id2rel = {}\n",
    "\n",
    "for k, v in rel2id_original.items():\n",
    "    if k == \"None\":\n",
    "        id2head_type[v] = \"None\"\n",
    "        id2tail_type[v] = \"None\"\n",
    "        id2rel[v] = \"None\"\n",
    "\n",
    "    else:\n",
    "        id2head_type[v] = k.split('/')[1]\n",
    "        id2tail_type[v] = k.split('/')[2]\n",
    "        id2rel[v] = k.split('/')[3]\n",
    "\n",
    "rel2id = {v: k for k, v in id2rel.items()}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # for the -1 dim of the logits, have the index of the max value, if the original shape is [b, s, 3] then the shape of the index is [b, s]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # make the predictions to a numpy array\n",
    "    if isinstance(predictions , torch.Tensor):\n",
    "        predictions  = predictions.numpy()\n",
    "\n",
    "    # if label is tensor, set the label to numpy\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.numpy()\n",
    "\n",
    "    # Total accuracy, ignore the -100 index in the labels\n",
    "    total_accuracy = np.mean(predictions[labels != -100] == labels[labels != -100])\n",
    "\n",
    "    # Accuracy for B (0)\n",
    "    accuracy_0 = np.mean(predictions[labels == 0] == 0)\n",
    "\n",
    "    \n",
    "    # Accuracy for in (1)\n",
    "    accuracy_1 = np.mean(predictions[labels == 1] == 1)\n",
    "    \n",
    "    # Accuracy for out (2)\n",
    "    accuracy_2 = np.mean(predictions[labels == 2] == 2)\n",
    "\n",
    "    # accuracy for each class\n",
    "\n",
    "    return {\n",
    "        \"precision_for_begin\": accuracy_0,\n",
    "        \"precision_for_in\": accuracy_1,\n",
    "        \"precision_for_out\": accuracy_2,\n",
    "        \"precision_for_all\": total_accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f26dc3a19714e154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:32:44.463762724Z",
     "start_time": "2023-10-04T15:32:44.298268789Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_for_begin': 0.0,\n",
       " 'precision_for_in': 0.5714285714285714,\n",
       " 'precision_for_out': 0.09803921568627451,\n",
       " 'precision_for_all': 0.14516129032258066}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics((outputs[1], tokenized_dataset['labels'][0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b00421072c935d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:34:08.995580447Z",
     "start_time": "2023-10-04T15:34:08.716572856Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# \n",
    "# from datasets import Dataset\n",
    "# \n",
    "# ner = 1\n",
    "# \n",
    "# test_relation_dict = {}\n",
    "# if ner:\n",
    "#     with open('nyt/data/test-sent-pre-process.json') as f:\n",
    "#         test_relation_dict = json.load(f)\n",
    "# \n",
    "#     test_dataset = Dataset.from_dict(\n",
    "#         {\n",
    "#             'text': test_relation_dict['text'],\n",
    "#             'entity': test_relation_dict['entity'],\n",
    "#             'relation': test_relation_dict['relation']\n",
    "#         }\n",
    "#     )\n",
    "# \n",
    "# else:\n",
    "#     pass\n",
    "# \n",
    "# \n",
    "# dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5da5eada141039e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:34:36.949937015Z",
     "start_time": "2023-10-04T15:34:34.029459251Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:02<00:00, 58.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# \n",
    "# output = {\"input_texts\": []}\n",
    "# test_labels = []\n",
    "# \n",
    "# for i in tqdm(range(0, len(dataset), 30)):\n",
    "#     result = pro_processing_ner(dataset[i:i+30])\n",
    "#     output[\"input_texts\"].extend(result[\"input_ids\"])\n",
    "#     test_labels.extend(result[\"labels\"])\n",
    "# \n",
    "# \n",
    "# from datasets import Dataset\n",
    "# \n",
    "# input_text_dataset = Dataset.from_dict(\n",
    "#     {\n",
    "#         'input_texts': output['input_texts'],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "458a3ca3890d43e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:35:35.614964901Z",
     "start_time": "2023-10-04T15:35:32.734454016Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d775f6337b54428aabb55a57f1ae7611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_dataset = input_text_dataset.map(lambda example: tokenizer(example['input_texts'], padding='max_length', truncation=True, max_length=512, add_special_tokens=False, pad_to_max_length=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9c8a74f5c9f674c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:35:51.643104238Z",
     "start_time": "2023-10-04T15:35:47.739411535Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b1d7e8bd3e47788c3a849765b810f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_dataset.remove_columns('input_texts')\n",
    "# tokenized_dataset = tokenized_dataset.add_column('labels', test_labels)\n",
    "# tokenized_dataset.save_to_disk('nyt/BERT_ED/test_data_RC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f4cd332c1f321c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:36:55.613039773Z",
     "start_time": "2023-10-04T15:36:55.314408843Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# valid_dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "valid_dataset = Dataset.load_from_disk('nyt/BERT_ED/test_data_RC')\n",
    "valid_dataset = valid_dataset.remove_columns('input_texts')\n",
    "\n",
    "\n",
    "# random sample 1 example from the test_dataset\n",
    "import random\n",
    "# have a random seed\n",
    "random.seed(65)\n",
    "\n",
    "\n",
    "random_test_index = random.randint(0, len(valid_dataset))\n",
    "\n",
    "\n",
    "# print(tokenizer.decode(tokenized_test_dataset[random_test_index * 96]['input_ids']))\n",
    "\n",
    "# output the length of tokenized_test_dataset[index]['input_ids'] except the padding tokens. the tokenized_test_dataset[index]['input_ids'] is tensor\n",
    "\n",
    "valid_dataset = valid_dataset[random_test_index : random_test_index + 10]\n",
    "\n",
    "valid_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'input_ids': valid_dataset['input_ids'],\n",
    "        'attention_mask': valid_dataset['attention_mask'],\n",
    "        'labels': valid_dataset['labels']\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d305203dd9fa4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2301c90e44f576cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:38:11.092980989Z",
     "start_time": "2023-10-04T15:37:57.789173103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/intermediate-sv/wandb/run-20231005_010748-w9ty8nlj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/BERT-intermediate/runs/w9ty8nlj' target=\"_blank\">ED_nyt_5epochs</a></strong> to <a href='https://wandb.ai/tian1995/BERT-intermediate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/BERT-intermediate' target=\"_blank\">https://wandb.ai/tian1995/BERT-intermediate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/BERT-intermediate/runs/w9ty8nlj' target=\"_blank\">https://wandb.ai/tian1995/BERT-intermediate/runs/w9ty8nlj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/BERT-intermediate/runs/w9ty8nlj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f137cf9fb20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"BERT-intermediate\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"ED_nyt_5epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbfae80f8b9e5659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:38:11.093438574Z",
     "start_time": "2023-10-04T15:38:10.573564098Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 56195\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a886e9ddaab9fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T15:47:55.576456059Z",
     "start_time": "2023-10-04T15:47:55.542094958Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='nyt/BERT_ED',\n",
    "    num_train_epochs=5,\n",
    "    auto_find_batch_size=True,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "class EntityDetectionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[1]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, 3),\n",
    "                        labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = EntityDetectionTrainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cf51140b7069f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-04T15:47:58.543170193Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11052' max='35125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11052/35125 1:25:23 < 3:06:02, 2.16 it/s, Epoch 1.57/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision For Begin</th>\n",
       "      <th>Precision For In</th>\n",
       "      <th>Precision For Out</th>\n",
       "      <th>Precision For All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.040267</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.992523</td>\n",
       "      <td>0.982238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1057af497a795",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "trainer.save_model(\"nyt/BERT_ED/model-5epochs\")\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"nyt/BERT_ED/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf85e0d8608da47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c8508fb42681a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "checkpoint = \"nyt/BERT_ED/model/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1fcdfaac2e719",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.config = self.bert.config\n",
    "\n",
    "    def forward(self, input_ids, \n",
    "                attention_mask=None, \n",
    "                token_type_ids=None,\n",
    "                labels=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,):\n",
    "        \n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,)\n",
    "        \n",
    "        pooled_output = outputs[0]\n",
    "\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                        labels.view(-1))\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "model = MultiLabelClassifier(num_labels=len(outputid2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f8d8649f8b4a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c6695369faa32",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.load_from_disk('nyt/BERT_ED/test_data_RC')\n",
    "# test_dataset = valid_dataset.remove_columns('input_texts')\n",
    "\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabe8f2195457c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=test_dataset['input_ids'][0:2], attention_mask=test_dataset['attention_mask'][0:2])\n",
    "    \n",
    "compute_metrics((outputs[0], test_dataset['labels'][0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618cab165256fea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "final_result = {\n",
    "        \"precision_for_begin\": [],\n",
    "        \"precision_for_in\": [],\n",
    "        \"precision_for_out\": [],\n",
    "        \"precision_for_all\": []\n",
    "    }\n",
    "\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "output_results = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # feed the actually_input to the model by 50 examples each time\n",
    "    for input_index in tqdm(range(0, len(test_dataset), 50)):\n",
    "        # print(input_index + 1, \" / \", len(tokenized_test_dataset))\n",
    "        input_ids =test_dataset[input_index : input_index+50][\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = test_dataset[input_index : input_index+50][\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # copy output[0] to cpu\n",
    "        pred = output[0].clone().cpu()\n",
    "        result = compute_metrics((pred, test_dataset[input_index : input_index+50][\"labels\"]))\n",
    "\n",
    "        for k, v in result.items():\n",
    "            final_result[k].append(v)\n",
    "\n",
    "\n",
    "result_sum = {}\n",
    "for k, v in final_result.items():  \n",
    "    result_sum[k] = sum(v) / 100\n",
    "\n",
    "print(result_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54acc3c17bf57c84",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# using matplotlib to plot the final_result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(result_sum.values())\n",
    "\n",
    "plt.xticks(range(len(result_sum)), result_sum.keys(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486c89b-ccbe-4dce-ba0e-3b025ce1091f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
